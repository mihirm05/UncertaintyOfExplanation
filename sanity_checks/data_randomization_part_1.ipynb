{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path  D:\\Mihir\\MT\\Code\\cifar\\GBP_explanation\\dropout\\saved_models\n",
      "cifar_working_path  D:\\Mihir\\MT\\Code\\cifar\\GBP_explanation\n",
      "src_path  D:\\Mihir\\MT\\Code\n",
      "explanation_path  D:\\Mihir\\MT\\Code\\cifar\\GBP_explanation\\dropout\\explanation_files\n",
      "explanation_heatmap_path  D:\\Mihir\\MT\\Code\\cifar\\GBP_explanation\\dropout\\explanation_heatmaps\n",
      "sanity_checks folder  D:\\Mihir\\MT\\Code\\cifar\\GBP_explanation\\dropout\\sanity_checks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), 'saved_models')\n",
    "cifar_working_dir = os.path.dirname(os.path.dirname(model_path))\n",
    "src_path = os.path.dirname(os.path.dirname(cifar_working_dir))\n",
    "explanation_path = os.path.join(os.path.dirname(model_path), 'explanation_files')\n",
    "explanation_heatmap_path = os.path.join(os.path.dirname(model_path), 'explanation_heatmaps')\n",
    "sanity_checks_output = os.path.join(os.path.join(os.getcwd(), 'sanity_checks'))\n",
    "\n",
    "\n",
    "print('model_path ', model_path)\n",
    "print('cifar_working_path ', cifar_working_dir)\n",
    "print('src_path ', src_path)\n",
    "print('explanation_path ', explanation_path)\n",
    "print('explanation_heatmap_path ', explanation_heatmap_path)\n",
    "print('sanity_checks folder ', sanity_checks_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "Eager execution enabled : True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import time\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import random \n",
    "import math\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "print(tf.__version__)\n",
    "print(f'Eager execution enabled : {tf.executing_eagerly()}')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,src_path)\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in train generator  50000\n",
      "number of examples in val generator  9500\n",
      "number of examples in test generator  500\n",
      "original train labels \n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "original train shape \n",
      "  (50000, 10)\n",
      "original test labels \n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "original test labels \n",
      "  (10000, 10)\n",
      "shuffled train labels \n",
      "  [6 2 2 ... 6 4 4]\n",
      "shuffled train shape \n",
      "  (50000,)\n",
      "shuffled test labels \n",
      "  [4 4 6 ... 4 8 5]\n",
      "shuffled test labels \n",
      "  (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), train_generator, validation_generator, test_generator = load_cifar(num_classes = 10,\n",
    "val_split = 0.95,\n",
    "rotation_range = 0.2,\n",
    "width_shift_range = 0.1, \n",
    "height_shift_range = 0.1,\n",
    "shear_range = 0.1,\n",
    "zoom_range = 0.1 ,\n",
    "horizontal_flip = True ,\n",
    "vertical_flip = False,\n",
    "rescale = 1/255.,\n",
    "train_batch_size=1024, \n",
    "val_batch_size=1024,\n",
    "test_batch_size=1024)\n",
    "\n",
    "\n",
    "# these sets are to be shuffled\n",
    "y_train_shuffled = np.argmax(y_train, axis=1)\n",
    "y_test_shuffled = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('original train labels \\n ', y_train)\n",
    "print('original train shape \\n ', y_train.shape)\n",
    "print('original test labels \\n ', y_test)\n",
    "print('original test labels \\n ', y_test.shape)\n",
    "\n",
    "np.random.shuffle(y_train_shuffled)\n",
    "np.random.shuffle(y_test_shuffled)\n",
    "\n",
    "print('shuffled train labels \\n ', y_train_shuffled)\n",
    "print('shuffled train shape \\n ', y_train_shuffled.shape)\n",
    "print('shuffled test labels \\n ', y_test_shuffled)\n",
    "print('shuffled test labels \\n ', y_test_shuffled.shape)\n",
    "\n",
    "# convert the shuffled labels to one hot encoding again \n",
    "y_train_shuffled = to_categorical(y_train_shuffled, num_classes=10)\n",
    "y_test_shuffled = to_categorical(y_test_shuffled, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions : StochasticModel & StochasticClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "Layer = tf.keras.layers.Layer\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "\n",
    "class StochasticDropout(Dropout):\n",
    "    \"\"\"\n",
    "        Applies Dropout to the input, independent of the training phase.\n",
    "        Used to easily implement MC-Dropout. It is a drop-in replacement for\n",
    "        the standard Keras Dropout layer, but note that this layer applies\n",
    "        dropout at the training and inference phases.\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n",
    "        super(StochasticDropout, self).__init__(rate, noise_shape, seed, **kwargs)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "            return K.dropout(inputs, self.rate, noise_shape, seed=self.seed)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        # this code is added so that the model can be saved even after making use of a custom layer \n",
    "        # https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object\n",
    "        config = super(StochasticDropout, self).get_config()\n",
    "        config.update({'rate': self.rate})\n",
    "        return config\n",
    "\n",
    "\n",
    "class StochasticModel:\n",
    "    \"\"\"\n",
    "        Stochastic model, requiring several forward passes to produce an estimate of the posterior predictive distribution.\n",
    "        This class just wraps a keras model to enable dropout at inference time.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_samples=10, **kwargs):\n",
    "        \"\"\"\n",
    "            Builds a stochastic model from a keras model. The model should already be trained.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def predict_samples(self, x, num_samples=None, batch_size=32, multi_output=False, **kwargs):\n",
    "        \"\"\"\n",
    "            Performs num_samples predictions using the model, and returns the produced output samples.\n",
    "        \"\"\"\n",
    "\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "\n",
    "        assert num_samples > 0\n",
    "        samples = [None] * num_samples\n",
    "\n",
    "        if \"verbose\" not in kwargs:\n",
    "            kwargs[\"verbose\"] = 0\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            samples[i] = self.model.predict(x, batch_size=1, **kwargs)\n",
    "\n",
    "        if multi_output:\n",
    "            return samples\n",
    "        else:\n",
    "            return np.array(samples)\n",
    "        \n",
    "\n",
    "class StochasticClassifier(StochasticModel):\n",
    "    def __init__(self, model, num_samples=10, **kwargs):\n",
    "        super().__init__(model, num_samples)\n",
    "        self.input = self.model.input\n",
    "        self.output = self.model.output\n",
    "\n",
    "    #essentially equivalent to call  \n",
    "    def __call__(self, inp, num_samples=None, **kwargs):\n",
    "        \"\"\"\n",
    "            Performs a prediction given input inp using MC Dropout, and returns the averaged probabilities of model output.\n",
    "        \"\"\"\n",
    "        samples = self.predict_samples(inp, num_samples, **kwargs)\n",
    "        mean_probs = np.mean(samples, axis=0)\n",
    "        print('in call function')\n",
    "        mean_probs = mean_probs / np.sum(mean_probs, axis=1, keepdims=True)\n",
    "\n",
    "        return samples, mean_probs\n",
    "\n",
    "\n",
    "    def predict_output(self, inp, num_samples=None, batch_size=32, **kwargs):\n",
    "        \"\"\"\n",
    "            Performs a prediction given input inp using MC Dropout, and returns the averaged probabilities of model output.\n",
    "        \"\"\"\n",
    "        samples = self.predict_samples(inp, num_samples, batch_size=batch_size, **kwargs)\n",
    "        print('Intermediate sample shape ', samples.shape)\n",
    "        mean_probs = np.mean(samples, axis=0)\n",
    "        mean_probs = mean_probs / np.sum(mean_probs, axis=1, keepdims=True)\n",
    "        print('Inside the predict_output() of stochasticclassifier class')\n",
    "\n",
    "        return samples, mean_probs\n",
    "\n",
    "    def save(self, path):\n",
    "        # stochaticmodel class does not have basic tf function like save(), summary(), get_layer() and so on \n",
    "        # https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object\n",
    "        self.model.save(path)\n",
    "\n",
    "    def summary(self):\n",
    "        return self.model.summary()  \n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        return self.model.evaluate(test_data)\n",
    "\n",
    "    def get_layer(self, name=None, index=None):\n",
    "        # the get_layer() has been taken from ths following source \n",
    "        # https://github.com/keras-team/keras/blob/v2.8.0/keras/engine/training.py#L2797-L2831\n",
    "    \n",
    "        \"\"\"Retrieves a layer based on either its name (unique) or index.\n",
    "        If `name` and `index` are both provided, `index` will take precedence.\n",
    "        Indices are based on order of horizontal graph traversal (bottom-up).\n",
    "        Args:\n",
    "            name: String, name of layer.\n",
    "            index: Integer, index of layer.\n",
    "        Returns:\n",
    "            A layer instance.\n",
    "        \"\"\"\n",
    "        # TODO(fchollet): We could build a dictionary based on layer names\n",
    "        # since they are constant, but we have not done that yet.\n",
    "        if index is not None and name is not None:\n",
    "            raise ValueError('Provide only a layer name or a layer index. Received: '\n",
    "                        f'index={index}, name={name}.')\n",
    "\n",
    "        if index is not None:\n",
    "            if len(self.layers) <= index:\n",
    "                raise ValueError(f'Was asked to retrieve layer at index {index}'\n",
    "                            f' but model only has {len(self.layers)}'\n",
    "                            ' layers.')\n",
    "            else:\n",
    "                return self.model.layers[index]\n",
    "\n",
    "        if name is not None:\n",
    "            for layer in self.model.layers:\n",
    "                if layer.name == name:\n",
    "                    return layer\n",
    "            raise ValueError(f'No such layer: {name}. Existing layers are: '\n",
    "                        f'{list(layer.name for layer in self.model.layers)}.')\n",
    "        raise ValueError('Provide either a layer name or layer index at '\n",
    "                     '`get_layer`.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 class map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_class_map = {0:'airplane',\n",
    "                   1:'automobile',\n",
    "                   2:'bird',\n",
    "                   3:'cat',\n",
    "                   4:'deer',\n",
    "                   5:'dog',\n",
    "                   6:'frog',\n",
    "                   7:'horse',\n",
    "                   8:'ship',\n",
    "                   9:'truck'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error if the custom object containing the stochastic dropout model not passed (Unknown layer: StochasticDropout. Please ensure this object is passed to the `custom_objects` argument)\n",
    "\n",
    "#import os\n",
    "#test = os.listdir(model_path)\n",
    "#model_checkpoints = []\n",
    "#model_name = []\n",
    "\n",
    "#for item in test:\n",
    "#    if item.endswith('.h5'):\n",
    "#        print('model name : ', item)\n",
    "#        model_name.append(item)\n",
    "        \n",
    "#print(model_name)\n",
    "#print(sorted(model_name))\n",
    "#best_model_path = model_name[-1]  # model naming template has the third element as epochs, save best will save the highest best performing epoch\n",
    "#print('best model path \\n', best_model_path)\n",
    "#trained_model = tf.keras.models.load_model(os.path.join(model_path,item), \n",
    "#                                   custom_objects={'StochasticDropout':StochasticDropout})\n",
    "\n",
    "#trained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef minivgg_dropout(input_shape, prob=0.25):\\n    model = Sequential()\\n    model.add(Conv2D(64, \\n                     kernel_size=(3, 3), \\n                     activation='relu', \\n                   padding='same', \\n                     input_shape=input_shape))\\n    model.add(BatchNormalization())\\n    model.add(MaxPooling2D((2, 2)))\\n\\n    model.add(Conv2D(128, \\n                     kernel_size=(3, 3), \\n                     padding='same', \\n                     activation='relu'))\\n    model.add(BatchNormalization())\\n    model.add(MaxPooling2D((2, 2)))\\n\\n    model.add(Conv2D(128, \\n                     kernel_size=(3, 3), \\n                     padding='same', \\n                     activation='relu'))\\n    model.add(BatchNormalization())\\n    model.add(MaxPooling2D((2, 2)))\\n    \\n    model.add(Flatten())\\n    model.add(StochasticDropout(prob))\\n    model.add(Dense(256, activation='relu'))\\n    model.add(StochasticDropout(prob))\\n    model.add(Dense(10, activation='softmax'))\\n    \\n    return model\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def minivgg_dropout(input_shape, prob=0.25):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, \n",
    "                     kernel_size=(3, 3), \n",
    "                     activation='relu', \n",
    "                   padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, \n",
    "                     kernel_size=(3, 3), \n",
    "                     padding='same', \n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, \n",
    "                     kernel_size=(3, 3), \n",
    "                     padding='same', \n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(StochasticDropout(prob))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(StochasticDropout(prob))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model (intentionally with incorrect labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minivgg_dropout(input_shape, prob=0.25):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, \n",
    "                     kernel_size=(3, 3), \n",
    "                     activation='relu', \n",
    "                     padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, \n",
    "                     kernel_size=(3, 3), \n",
    "                     padding='same', \n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, \n",
    "                     kernel_size=(3, 3), \n",
    "                     padding='same', \n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(StochasticDropout(prob))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(StochasticDropout(prob))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "19/19 [==============================] - 6s 170ms/step - loss: 3.0988 - accuracy: 0.0974 - val_loss: 2.3163 - val_accuracy: 0.1018\n",
      "Epoch 2/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.3095 - accuracy: 0.1003 - val_loss: 2.6439 - val_accuracy: 0.1007\n",
      "Epoch 3/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.3033 - accuracy: 0.1039 - val_loss: 2.8338 - val_accuracy: 0.1002\n",
      "Epoch 4/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.3024 - accuracy: 0.1041 - val_loss: 2.7929 - val_accuracy: 0.0994\n",
      "Epoch 5/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.3021 - accuracy: 0.1055 - val_loss: 2.7127 - val_accuracy: 0.1004\n",
      "Epoch 6/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.3025 - accuracy: 0.1034 - val_loss: 2.6069 - val_accuracy: 0.0976\n",
      "Epoch 7/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.3013 - accuracy: 0.1069 - val_loss: 2.5159 - val_accuracy: 0.0994\n",
      "Epoch 8/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.3012 - accuracy: 0.1067 - val_loss: 2.5056 - val_accuracy: 0.0993\n",
      "Epoch 9/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.3004 - accuracy: 0.1057 - val_loss: 2.4579 - val_accuracy: 0.1022\n",
      "Epoch 10/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2999 - accuracy: 0.1077 - val_loss: 2.4203 - val_accuracy: 0.1029\n",
      "Epoch 11/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2997 - accuracy: 0.1109 - val_loss: 2.4525 - val_accuracy: 0.0972\n",
      "Epoch 12/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 2.2982 - accuracy: 0.1090 - val_loss: 2.4447 - val_accuracy: 0.0971\n",
      "Epoch 13/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2967 - accuracy: 0.1120 - val_loss: 2.4153 - val_accuracy: 0.1006\n",
      "Epoch 14/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2950 - accuracy: 0.1138 - val_loss: 2.4068 - val_accuracy: 0.0936\n",
      "Epoch 15/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2946 - accuracy: 0.1173 - val_loss: 2.4152 - val_accuracy: 0.0962\n",
      "Epoch 16/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.2913 - accuracy: 0.1192 - val_loss: 2.4036 - val_accuracy: 0.1002\n",
      "Epoch 17/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.2891 - accuracy: 0.1214 - val_loss: 2.3837 - val_accuracy: 0.1027\n",
      "Epoch 18/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.2862 - accuracy: 0.1217 - val_loss: 2.4048 - val_accuracy: 0.0967\n",
      "Epoch 19/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2817 - accuracy: 0.1238 - val_loss: 2.3721 - val_accuracy: 0.1020\n",
      "Epoch 20/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.2787 - accuracy: 0.1265 - val_loss: 2.3586 - val_accuracy: 0.0997\n",
      "Epoch 21/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2724 - accuracy: 0.1335 - val_loss: 2.3789 - val_accuracy: 0.1022\n",
      "Epoch 22/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2655 - accuracy: 0.1363 - val_loss: 2.3634 - val_accuracy: 0.0982\n",
      "Epoch 23/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2618 - accuracy: 0.1413 - val_loss: 2.3992 - val_accuracy: 0.0995\n",
      "Epoch 24/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2551 - accuracy: 0.1455 - val_loss: 2.3712 - val_accuracy: 0.0967\n",
      "Epoch 25/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2473 - accuracy: 0.1502 - val_loss: 2.3916 - val_accuracy: 0.0981\n",
      "Epoch 26/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2388 - accuracy: 0.1536 - val_loss: 2.3784 - val_accuracy: 0.0969\n",
      "Epoch 27/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.2258 - accuracy: 0.1617 - val_loss: 2.3991 - val_accuracy: 0.0952\n",
      "Epoch 28/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.2176 - accuracy: 0.1652 - val_loss: 2.3969 - val_accuracy: 0.0999\n",
      "Epoch 29/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.1996 - accuracy: 0.1775 - val_loss: 2.3950 - val_accuracy: 0.1014\n",
      "Epoch 30/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.1903 - accuracy: 0.1818 - val_loss: 2.4438 - val_accuracy: 0.0974\n",
      "Epoch 31/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.1717 - accuracy: 0.1900 - val_loss: 2.4207 - val_accuracy: 0.1040\n",
      "Epoch 32/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.1541 - accuracy: 0.1999 - val_loss: 2.6909 - val_accuracy: 0.0977\n",
      "Epoch 33/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.1337 - accuracy: 0.2062 - val_loss: 2.5533 - val_accuracy: 0.0966\n",
      "Epoch 34/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.1100 - accuracy: 0.2184 - val_loss: 2.5701 - val_accuracy: 0.0980\n",
      "Epoch 35/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.0855 - accuracy: 0.2294 - val_loss: 2.6092 - val_accuracy: 0.0946\n",
      "Epoch 36/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.0667 - accuracy: 0.2397 - val_loss: 2.6727 - val_accuracy: 0.0931\n",
      "Epoch 37/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.0313 - accuracy: 0.2555 - val_loss: 2.7662 - val_accuracy: 0.0966\n",
      "Epoch 38/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.0032 - accuracy: 0.2616 - val_loss: 2.7525 - val_accuracy: 0.1000\n",
      "Epoch 39/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.9732 - accuracy: 0.2818 - val_loss: 2.8004 - val_accuracy: 0.1033\n",
      "Epoch 40/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.9364 - accuracy: 0.2925 - val_loss: 3.0248 - val_accuracy: 0.1001\n",
      "Epoch 41/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.8967 - accuracy: 0.3083 - val_loss: 2.7224 - val_accuracy: 0.1003\n",
      "Epoch 42/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.8693 - accuracy: 0.3205 - val_loss: 3.1177 - val_accuracy: 0.0947\n",
      "Epoch 43/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.8403 - accuracy: 0.3334 - val_loss: 2.9790 - val_accuracy: 0.0983\n",
      "Epoch 44/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.7985 - accuracy: 0.3503 - val_loss: 2.8858 - val_accuracy: 0.0963\n",
      "Epoch 45/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.7561 - accuracy: 0.3660 - val_loss: 3.2337 - val_accuracy: 0.1009\n",
      "Epoch 46/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.7196 - accuracy: 0.3804 - val_loss: 3.0329 - val_accuracy: 0.0961\n",
      "Epoch 47/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.6839 - accuracy: 0.3937 - val_loss: 3.2160 - val_accuracy: 0.1046\n",
      "Epoch 48/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.6394 - accuracy: 0.4089 - val_loss: 3.4265 - val_accuracy: 0.1006\n",
      "Epoch 49/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.6040 - accuracy: 0.4243 - val_loss: 3.4881 - val_accuracy: 0.1034\n",
      "Epoch 50/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.5743 - accuracy: 0.4350 - val_loss: 3.5822 - val_accuracy: 0.0977\n",
      "Epoch 51/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.5467 - accuracy: 0.4472 - val_loss: 3.4763 - val_accuracy: 0.0987\n",
      "Epoch 52/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.5042 - accuracy: 0.4607 - val_loss: 3.3714 - val_accuracy: 0.0990\n",
      "Epoch 53/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.4646 - accuracy: 0.4781 - val_loss: 3.4537 - val_accuracy: 0.0991\n",
      "Epoch 54/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.4270 - accuracy: 0.4924 - val_loss: 3.1165 - val_accuracy: 0.1020\n",
      "Epoch 55/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.3971 - accuracy: 0.5041 - val_loss: 3.5110 - val_accuracy: 0.1050\n",
      "Epoch 56/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.3619 - accuracy: 0.5164 - val_loss: 3.9463 - val_accuracy: 0.0990\n",
      "Epoch 57/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.3282 - accuracy: 0.5268 - val_loss: 3.9379 - val_accuracy: 0.0966\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 90ms/step - loss: 1.3011 - accuracy: 0.5359 - val_loss: 3.6739 - val_accuracy: 0.1024\n",
      "Epoch 59/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.2682 - accuracy: 0.5499 - val_loss: 3.6547 - val_accuracy: 0.1024\n",
      "Epoch 60/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.2414 - accuracy: 0.5594 - val_loss: 4.1101 - val_accuracy: 0.1036\n",
      "Epoch 61/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.2146 - accuracy: 0.5702 - val_loss: 4.1095 - val_accuracy: 0.1042\n",
      "Epoch 62/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.1920 - accuracy: 0.5756 - val_loss: 3.8057 - val_accuracy: 0.1005\n",
      "Epoch 63/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.1662 - accuracy: 0.5870 - val_loss: 3.7611 - val_accuracy: 0.0981\n",
      "Epoch 64/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.1355 - accuracy: 0.6007 - val_loss: 3.6821 - val_accuracy: 0.0988\n",
      "Epoch 65/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.1210 - accuracy: 0.6063 - val_loss: 4.2693 - val_accuracy: 0.1062\n",
      "Epoch 66/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 1.0860 - accuracy: 0.6184 - val_loss: 4.1519 - val_accuracy: 0.1010\n",
      "Epoch 67/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.0687 - accuracy: 0.6247 - val_loss: 4.0691 - val_accuracy: 0.0965\n",
      "Epoch 68/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.0408 - accuracy: 0.6374 - val_loss: 4.2147 - val_accuracy: 0.0983\n",
      "Epoch 69/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.0312 - accuracy: 0.6387 - val_loss: 4.0596 - val_accuracy: 0.0955\n",
      "Epoch 70/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.0124 - accuracy: 0.6443 - val_loss: 4.4977 - val_accuracy: 0.1022\n",
      "Epoch 71/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.0014 - accuracy: 0.6525 - val_loss: 4.4532 - val_accuracy: 0.1006\n",
      "Epoch 72/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.9731 - accuracy: 0.6599 - val_loss: 4.2908 - val_accuracy: 0.1022\n",
      "Epoch 73/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.9608 - accuracy: 0.6667 - val_loss: 4.0905 - val_accuracy: 0.1010\n",
      "Epoch 74/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.9263 - accuracy: 0.6753 - val_loss: 4.7729 - val_accuracy: 0.1027\n",
      "Epoch 75/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.9237 - accuracy: 0.6779 - val_loss: 4.6703 - val_accuracy: 0.1002\n",
      "Epoch 76/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.9093 - accuracy: 0.6802 - val_loss: 4.3047 - val_accuracy: 0.1031\n",
      "Epoch 77/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.8922 - accuracy: 0.6866 - val_loss: 4.4979 - val_accuracy: 0.1014\n",
      "Epoch 78/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.8727 - accuracy: 0.6931 - val_loss: 4.4276 - val_accuracy: 0.1054\n",
      "Epoch 79/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.8503 - accuracy: 0.7035 - val_loss: 4.3698 - val_accuracy: 0.0978\n",
      "Epoch 80/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.8431 - accuracy: 0.7057 - val_loss: 4.3156 - val_accuracy: 0.0998\n",
      "Epoch 81/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.8228 - accuracy: 0.7142 - val_loss: 3.9292 - val_accuracy: 0.1046\n",
      "Epoch 82/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.8135 - accuracy: 0.7149 - val_loss: 4.3002 - val_accuracy: 0.0965\n",
      "Epoch 83/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.8104 - accuracy: 0.7180 - val_loss: 4.5051 - val_accuracy: 0.1021\n",
      "Epoch 84/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.7813 - accuracy: 0.7293 - val_loss: 4.2996 - val_accuracy: 0.1019\n",
      "Epoch 85/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.7748 - accuracy: 0.7311 - val_loss: 4.6877 - val_accuracy: 0.1029\n",
      "Epoch 86/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.7678 - accuracy: 0.7334 - val_loss: 4.6499 - val_accuracy: 0.0994\n",
      "Epoch 87/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.7470 - accuracy: 0.7431 - val_loss: 4.7507 - val_accuracy: 0.0980\n",
      "Epoch 88/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.7463 - accuracy: 0.7414 - val_loss: 5.2333 - val_accuracy: 0.0973\n",
      "Epoch 89/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.7409 - accuracy: 0.7434 - val_loss: 4.5569 - val_accuracy: 0.1050\n",
      "Epoch 90/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.7317 - accuracy: 0.7487 - val_loss: 4.8041 - val_accuracy: 0.0966\n",
      "Epoch 91/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.7150 - accuracy: 0.7537 - val_loss: 4.6142 - val_accuracy: 0.0991\n",
      "Epoch 92/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.6912 - accuracy: 0.7603 - val_loss: 4.7109 - val_accuracy: 0.1035\n",
      "Epoch 93/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6918 - accuracy: 0.7622 - val_loss: 5.0691 - val_accuracy: 0.1010\n",
      "Epoch 94/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6811 - accuracy: 0.7648 - val_loss: 5.0328 - val_accuracy: 0.0972\n",
      "Epoch 95/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.6832 - accuracy: 0.7644 - val_loss: 4.7485 - val_accuracy: 0.0970\n",
      "Epoch 96/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.6600 - accuracy: 0.7733 - val_loss: 4.9190 - val_accuracy: 0.0969\n",
      "Epoch 97/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.6538 - accuracy: 0.7738 - val_loss: 5.1525 - val_accuracy: 0.1005\n",
      "Epoch 98/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.6548 - accuracy: 0.7751 - val_loss: 4.7440 - val_accuracy: 0.0993\n",
      "Epoch 99/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.6451 - accuracy: 0.7781 - val_loss: 4.9665 - val_accuracy: 0.0998\n",
      "Epoch 100/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6435 - accuracy: 0.7794 - val_loss: 4.9651 - val_accuracy: 0.1025\n",
      "Epoch 101/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.6317 - accuracy: 0.7834 - val_loss: 5.0739 - val_accuracy: 0.1025\n",
      "Epoch 102/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6127 - accuracy: 0.7870 - val_loss: 5.4821 - val_accuracy: 0.1030\n",
      "Epoch 103/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.6202 - accuracy: 0.7881 - val_loss: 4.9416 - val_accuracy: 0.0949\n",
      "Epoch 104/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5994 - accuracy: 0.7950 - val_loss: 4.6796 - val_accuracy: 0.1003\n",
      "Epoch 105/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.6091 - accuracy: 0.7899 - val_loss: 5.6324 - val_accuracy: 0.0981\n",
      "Epoch 106/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.6073 - accuracy: 0.7938 - val_loss: 5.5189 - val_accuracy: 0.1038\n",
      "Epoch 107/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5842 - accuracy: 0.7981 - val_loss: 5.3789 - val_accuracy: 0.0949\n",
      "Epoch 108/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.5798 - accuracy: 0.7990 - val_loss: 5.1632 - val_accuracy: 0.1012\n",
      "Epoch 109/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.5759 - accuracy: 0.8016 - val_loss: 5.2620 - val_accuracy: 0.1002\n",
      "Epoch 110/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5887 - accuracy: 0.7977 - val_loss: 5.2549 - val_accuracy: 0.1058\n",
      "Epoch 111/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5684 - accuracy: 0.8045 - val_loss: 4.9187 - val_accuracy: 0.0996\n",
      "Epoch 112/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5641 - accuracy: 0.8079 - val_loss: 5.3277 - val_accuracy: 0.1002\n",
      "Epoch 113/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5619 - accuracy: 0.8072 - val_loss: 5.4495 - val_accuracy: 0.1036\n",
      "Epoch 114/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5467 - accuracy: 0.8126 - val_loss: 4.6861 - val_accuracy: 0.1015\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5447 - accuracy: 0.8142 - val_loss: 5.1986 - val_accuracy: 0.1002\n",
      "Epoch 116/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5415 - accuracy: 0.8143 - val_loss: 4.8714 - val_accuracy: 0.1016\n",
      "Epoch 117/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5392 - accuracy: 0.8162 - val_loss: 5.1344 - val_accuracy: 0.0990\n",
      "Epoch 118/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5303 - accuracy: 0.8173 - val_loss: 5.5159 - val_accuracy: 0.0989\n",
      "Epoch 119/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5222 - accuracy: 0.8216 - val_loss: 4.8278 - val_accuracy: 0.1002\n",
      "Epoch 120/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5156 - accuracy: 0.8240 - val_loss: 5.3995 - val_accuracy: 0.1013\n",
      "Epoch 121/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5139 - accuracy: 0.8239 - val_loss: 5.1945 - val_accuracy: 0.1030\n",
      "Epoch 122/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5083 - accuracy: 0.8277 - val_loss: 5.6066 - val_accuracy: 0.0979\n",
      "Epoch 123/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5084 - accuracy: 0.8263 - val_loss: 5.1338 - val_accuracy: 0.1011\n",
      "Epoch 124/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5100 - accuracy: 0.8250 - val_loss: 5.4776 - val_accuracy: 0.1002\n",
      "Epoch 125/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4972 - accuracy: 0.8305 - val_loss: 5.5511 - val_accuracy: 0.1002\n",
      "Epoch 126/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4994 - accuracy: 0.8298 - val_loss: 5.4744 - val_accuracy: 0.1030\n",
      "Epoch 127/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5050 - accuracy: 0.8258 - val_loss: 5.6495 - val_accuracy: 0.1026\n",
      "Epoch 128/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.4827 - accuracy: 0.8355 - val_loss: 5.5358 - val_accuracy: 0.0996\n",
      "Epoch 129/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.4892 - accuracy: 0.8334 - val_loss: 5.6335 - val_accuracy: 0.1026\n",
      "Epoch 130/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4899 - accuracy: 0.8338 - val_loss: 5.5105 - val_accuracy: 0.0998\n",
      "Epoch 131/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4878 - accuracy: 0.8340 - val_loss: 5.8489 - val_accuracy: 0.1011\n",
      "Epoch 132/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4726 - accuracy: 0.8383 - val_loss: 5.6168 - val_accuracy: 0.1002\n",
      "Epoch 133/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.4640 - accuracy: 0.8419 - val_loss: 5.2743 - val_accuracy: 0.1004\n",
      "Epoch 134/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4663 - accuracy: 0.8411 - val_loss: 6.2640 - val_accuracy: 0.0987\n",
      "Epoch 135/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4733 - accuracy: 0.8416 - val_loss: 5.7193 - val_accuracy: 0.0962\n",
      "Epoch 136/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4566 - accuracy: 0.8424 - val_loss: 5.4857 - val_accuracy: 0.0992\n",
      "Epoch 137/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4574 - accuracy: 0.8470 - val_loss: 5.6875 - val_accuracy: 0.0990\n",
      "Epoch 138/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4486 - accuracy: 0.8477 - val_loss: 5.2284 - val_accuracy: 0.1027\n",
      "Epoch 139/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4466 - accuracy: 0.8481 - val_loss: 5.4494 - val_accuracy: 0.1081\n",
      "Epoch 140/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4413 - accuracy: 0.8506 - val_loss: 6.0469 - val_accuracy: 0.1017\n",
      "Epoch 141/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4434 - accuracy: 0.8498 - val_loss: 5.8906 - val_accuracy: 0.1015\n",
      "Epoch 142/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.4383 - accuracy: 0.8518 - val_loss: 5.7799 - val_accuracy: 0.1059\n",
      "Epoch 143/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4392 - accuracy: 0.8511 - val_loss: 5.5916 - val_accuracy: 0.1038\n",
      "Epoch 144/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4355 - accuracy: 0.8526 - val_loss: 5.6463 - val_accuracy: 0.1002\n",
      "Epoch 145/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4287 - accuracy: 0.8540 - val_loss: 5.4031 - val_accuracy: 0.1020\n",
      "Epoch 146/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4236 - accuracy: 0.8567 - val_loss: 5.5605 - val_accuracy: 0.0988\n",
      "Epoch 147/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4315 - accuracy: 0.8534 - val_loss: 5.8935 - val_accuracy: 0.1020\n",
      "Epoch 148/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4297 - accuracy: 0.8521 - val_loss: 5.4815 - val_accuracy: 0.1013\n",
      "Epoch 149/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4194 - accuracy: 0.8575 - val_loss: 6.2386 - val_accuracy: 0.0974\n",
      "Epoch 150/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.4198 - accuracy: 0.8560 - val_loss: 5.8316 - val_accuracy: 0.1030\n",
      "Epoch 151/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4226 - accuracy: 0.8563 - val_loss: 6.4179 - val_accuracy: 0.1010\n",
      "Epoch 152/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4159 - accuracy: 0.8570 - val_loss: 6.2746 - val_accuracy: 0.1011\n",
      "Epoch 153/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4151 - accuracy: 0.8577 - val_loss: 5.5385 - val_accuracy: 0.1075\n",
      "Epoch 154/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4218 - accuracy: 0.8574 - val_loss: 5.7441 - val_accuracy: 0.0989\n",
      "Epoch 155/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4094 - accuracy: 0.8611 - val_loss: 5.7741 - val_accuracy: 0.0990\n",
      "Epoch 156/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4006 - accuracy: 0.8651 - val_loss: 5.7756 - val_accuracy: 0.1034\n",
      "Epoch 157/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.4079 - accuracy: 0.8629 - val_loss: 6.1409 - val_accuracy: 0.1021\n",
      "Epoch 158/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.4078 - accuracy: 0.8605 - val_loss: 6.2551 - val_accuracy: 0.1034\n",
      "Epoch 159/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3996 - accuracy: 0.8670 - val_loss: 6.0963 - val_accuracy: 0.0970\n",
      "Epoch 160/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3997 - accuracy: 0.8658 - val_loss: 5.6320 - val_accuracy: 0.1012\n",
      "Epoch 161/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3907 - accuracy: 0.8679 - val_loss: 5.4887 - val_accuracy: 0.0998\n",
      "Epoch 162/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3836 - accuracy: 0.8703 - val_loss: 5.9567 - val_accuracy: 0.1001\n",
      "Epoch 163/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3848 - accuracy: 0.8697 - val_loss: 5.8802 - val_accuracy: 0.1006\n",
      "Epoch 164/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3916 - accuracy: 0.8668 - val_loss: 6.1342 - val_accuracy: 0.1072\n",
      "Epoch 165/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.3802 - accuracy: 0.8722 - val_loss: 5.8715 - val_accuracy: 0.0989\n",
      "Epoch 166/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3831 - accuracy: 0.8722 - val_loss: 5.9477 - val_accuracy: 0.0987\n",
      "Epoch 167/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.3815 - accuracy: 0.8724 - val_loss: 6.0433 - val_accuracy: 0.0986\n",
      "Epoch 168/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3799 - accuracy: 0.8721 - val_loss: 6.0345 - val_accuracy: 0.0972\n",
      "Epoch 169/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3824 - accuracy: 0.8691 - val_loss: 5.9815 - val_accuracy: 0.1002\n",
      "Epoch 170/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3734 - accuracy: 0.8735 - val_loss: 5.6441 - val_accuracy: 0.1010\n",
      "Epoch 171/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3788 - accuracy: 0.8726 - val_loss: 6.0969 - val_accuracy: 0.0983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3822 - accuracy: 0.8709 - val_loss: 6.4267 - val_accuracy: 0.1010\n",
      "Epoch 173/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3687 - accuracy: 0.8753 - val_loss: 6.1461 - val_accuracy: 0.1010\n",
      "Epoch 174/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3729 - accuracy: 0.8725 - val_loss: 6.3482 - val_accuracy: 0.0998\n",
      "Epoch 175/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.3704 - accuracy: 0.8751 - val_loss: 6.0170 - val_accuracy: 0.1015\n",
      "Epoch 176/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3505 - accuracy: 0.8798 - val_loss: 6.1502 - val_accuracy: 0.1061\n",
      "Epoch 177/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3651 - accuracy: 0.8782 - val_loss: 6.0497 - val_accuracy: 0.1005\n",
      "Epoch 178/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3641 - accuracy: 0.8786 - val_loss: 6.0073 - val_accuracy: 0.1040\n",
      "Epoch 179/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3595 - accuracy: 0.8782 - val_loss: 5.6238 - val_accuracy: 0.1003\n",
      "Epoch 180/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3581 - accuracy: 0.8779 - val_loss: 6.4431 - val_accuracy: 0.1026\n",
      "Epoch 181/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3577 - accuracy: 0.8782 - val_loss: 6.2353 - val_accuracy: 0.1034\n",
      "Epoch 182/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3564 - accuracy: 0.8813 - val_loss: 6.2122 - val_accuracy: 0.1007\n",
      "Epoch 183/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.3471 - accuracy: 0.8835 - val_loss: 5.9293 - val_accuracy: 0.1017\n",
      "Epoch 184/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3452 - accuracy: 0.8851 - val_loss: 6.1031 - val_accuracy: 0.1031\n",
      "Epoch 185/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3478 - accuracy: 0.8829 - val_loss: 5.4409 - val_accuracy: 0.0987\n",
      "Epoch 186/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3512 - accuracy: 0.8826 - val_loss: 5.7198 - val_accuracy: 0.1019\n",
      "Epoch 187/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3542 - accuracy: 0.8799 - val_loss: 6.1705 - val_accuracy: 0.1010\n",
      "Epoch 188/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.3417 - accuracy: 0.8843 - val_loss: 6.0382 - val_accuracy: 0.1022\n",
      "Epoch 189/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3440 - accuracy: 0.8853 - val_loss: 5.9872 - val_accuracy: 0.1005\n",
      "Epoch 190/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.3439 - accuracy: 0.8838 - val_loss: 5.9334 - val_accuracy: 0.1029\n",
      "Epoch 191/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.3407 - accuracy: 0.8857 - val_loss: 5.8063 - val_accuracy: 0.1008\n",
      "Epoch 192/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3463 - accuracy: 0.8833 - val_loss: 6.1262 - val_accuracy: 0.1048\n",
      "Epoch 193/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3403 - accuracy: 0.8849 - val_loss: 6.5347 - val_accuracy: 0.1022\n",
      "Epoch 194/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.3444 - accuracy: 0.8862 - val_loss: 6.1765 - val_accuracy: 0.1005\n",
      "Epoch 195/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3386 - accuracy: 0.8859 - val_loss: 5.9840 - val_accuracy: 0.1018\n",
      "Epoch 196/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3400 - accuracy: 0.8863 - val_loss: 5.8021 - val_accuracy: 0.1024\n",
      "Epoch 197/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.3318 - accuracy: 0.8892 - val_loss: 5.9227 - val_accuracy: 0.0998\n",
      "Epoch 198/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.3279 - accuracy: 0.8887 - val_loss: 6.0455 - val_accuracy: 0.1019\n",
      "Epoch 199/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.3336 - accuracy: 0.8875 - val_loss: 6.3705 - val_accuracy: 0.1036\n",
      "Epoch 200/200\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.3338 - accuracy: 0.8871 - val_loss: 6.2775 - val_accuracy: 0.1006\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs=200\n",
    "num_samples=10 \n",
    "prob=0.25\n",
    "random_model = minivgg_dropout(input_shape=x_train.shape[1:], prob=prob)\n",
    "\n",
    "history = random_model.fit(x_train, \n",
    "                                     y_train_shuffled,\n",
    "                                     batch_size=2048,\n",
    "                                     validation_split=0.25,\n",
    "                                     validation_batch_size=2048,\n",
    "                                     epochs=epochs)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss vs epochs')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy vs epochs')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "random_model.save(os.path.join(os.getcwd(), 'randomized_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBP Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef guided_bp(test_input, test_label, model, layer_name):\\n    @tf.custom_gradient\\n    def guidedRelu(x):\\n        #https://towardsdatascience.com/how-to-visually-explain-any-cnn-based-models-80e0975ce57\\n        def grad(dy):\\n            return tf.cast(dy>0,'float32') * tf.cast(x>0, 'float32') * dy\\n        return tf.nn.relu(x), grad\\n\\n    normalized_gb_list = [] # collect the noramlized heatmaps of all the samples in a list \\n    predicted_class_list = [] # to collect the prediction of all the samples in a list \\n    LAYER_NAME = layer_name\\n\\n    # create a model till last convolutional layers to have the best compromise between high-level semantics and detailed spatial information\\n    gb_model = tf.keras.models.Model(inputs = [model.input],    \\n                                     outputs = [model.get_layer(LAYER_NAME).output])\\n    \\n    #print('list of gbp_model layer name : ')\\n    #for i in gb_model.layers:\\n    #    print('layer_name_ ', i.name)\\n    \\n    #gb_model.summary()\\n    \\n    layer_dict = [layer for layer in gb_model.layers[1:] if hasattr(layer,'activation')]\\n    #value = model.get_layer(LAYER_NAME).output.shape[-1]  # to compute the number of channels in the volume at a particular layer \\n\\n    for layer in layer_dict:\\n        if layer.activation == tf.keras.activations.relu:\\n            layer.activation = guidedRelu\\n\\n\\n    with tf.GradientTape() as tape:\\n        inputs = tf.cast(test_input, tf.float32)\\n        tape.watch(inputs)\\n\\n        #outputs = gb_model.predict(inputs) \\n        outputs = gb_model(inputs)\\n        #print('model prediction vector \\n', outputs)\\n        #print('model prediction ', np.argmax(outputs))\\n        # essentially model() and model.predict() yield the same output with the difference that \\n        # https://stackoverflow.com/questions/55308425/difference-between-modelx-and-model-predictx-in-keras#:~:text=The%20format%20may%20be%20different,its%20structure%2C%20weights%20and%20biases.\\n        #Today given a model in tensorflow 2.0 (built using the keras library), #out_np = model.predict(x) provides a numpy array which can, as an example, be printed with print(out_np).\\n        #On the other hand,out_tf = model(x) results into a tensorflow object, wich can be converted to a numpy array with .numpy()\\n        \\n        predicted_class = np.argmax(outputs)\\n        max_of_predicted_logits = outputs[:, predicted_class]\\n        #print('max of predicted logits : ', max_of_predicted_logits)\\n        print('prediction : ', predicted_class)\\n        print('GT : ', np.argmax(test_label))\\n        \\n        class_label = np.argmax(test_label)\\n        desired_class_logit = outputs[:, class_label]\\n        \\n    #print('outputs[:, predicted_class] : ', outputs[:, predicted_class])\\n    grads = tape.gradient(max_of_predicted_logits, inputs)[0]# selecting that unit in dense layer that has the maximum activation for a given input,,,, if[0] not selected then shape of grads is: [1,32,32,3] \\n    #grads = tape.gradient(desired_class_logit, inputs)[0]\\n    #print('grads shape : \\n', grads.shape)\\n\\n    return grads, predicted_class\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def guided_bp(test_input, test_label, model, layer_name):\n",
    "    @tf.custom_gradient\n",
    "    def guidedRelu(x):\n",
    "        #https://towardsdatascience.com/how-to-visually-explain-any-cnn-based-models-80e0975ce57\n",
    "        def grad(dy):\n",
    "            return tf.cast(dy>0,'float32') * tf.cast(x>0, 'float32') * dy\n",
    "        return tf.nn.relu(x), grad\n",
    "\n",
    "    normalized_gb_list = [] # collect the noramlized heatmaps of all the samples in a list \n",
    "    predicted_class_list = [] # to collect the prediction of all the samples in a list \n",
    "    LAYER_NAME = layer_name\n",
    "\n",
    "    # create a model till last convolutional layers to have the best compromise between high-level semantics and detailed spatial information\n",
    "    gb_model = tf.keras.models.Model(inputs = [model.input],    \n",
    "                                     outputs = [model.get_layer(LAYER_NAME).output])\n",
    "    \n",
    "    #print('list of gbp_model layer name : ')\n",
    "    #for i in gb_model.layers:\n",
    "    #    print('layer_name_ ', i.name)\n",
    "    \n",
    "    #gb_model.summary()\n",
    "    \n",
    "    layer_dict = [layer for layer in gb_model.layers[1:] if hasattr(layer,'activation')]\n",
    "    #value = model.get_layer(LAYER_NAME).output.shape[-1]  # to compute the number of channels in the volume at a particular layer \n",
    "\n",
    "    for layer in layer_dict:\n",
    "        if layer.activation == tf.keras.activations.relu:\n",
    "            layer.activation = guidedRelu\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        inputs = tf.cast(test_input, tf.float32)\n",
    "        tape.watch(inputs)\n",
    "\n",
    "        #outputs = gb_model.predict(inputs) \n",
    "        outputs = gb_model(inputs)\n",
    "        #print('model prediction vector \\n', outputs)\n",
    "        #print('model prediction ', np.argmax(outputs))\n",
    "        # essentially model() and model.predict() yield the same output with the difference that \n",
    "        # https://stackoverflow.com/questions/55308425/difference-between-modelx-and-model-predictx-in-keras#:~:text=The%20format%20may%20be%20different,its%20structure%2C%20weights%20and%20biases.\n",
    "        #Today given a model in tensorflow 2.0 (built using the keras library), #out_np = model.predict(x) provides a numpy array which can, as an example, be printed with print(out_np).\n",
    "        #On the other hand,out_tf = model(x) results into a tensorflow object, wich can be converted to a numpy array with .numpy()\n",
    "        \n",
    "        predicted_class = np.argmax(outputs)\n",
    "        max_of_predicted_logits = outputs[:, predicted_class]\n",
    "        #print('max of predicted logits : ', max_of_predicted_logits)\n",
    "        print('prediction : ', predicted_class)\n",
    "        print('GT : ', np.argmax(test_label))\n",
    "        \n",
    "        class_label = np.argmax(test_label)\n",
    "        desired_class_logit = outputs[:, class_label]\n",
    "        \n",
    "    #print('outputs[:, predicted_class] : ', outputs[:, predicted_class])\n",
    "    grads = tape.gradient(max_of_predicted_logits, inputs)[0]# selecting that unit in dense layer that has the maximum activation for a given input,,,, if[0] not selected then shape of grads is: [1,32,32,3] \n",
    "    #grads = tape.gradient(desired_class_logit, inputs)[0]\n",
    "    #print('grads shape : \\n', grads.shape)\n",
    "\n",
    "    return grads, predicted_class\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IG Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-11-723afb4200cc>, line 234)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-723afb4200cc>\"\u001b[1;36m, line \u001b[1;32m234\u001b[0m\n\u001b[1;33m    ''''\u001b[0m\n\u001b[1;37m        \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "class IntegratedGradient:\n",
    "    def visualize_input_to_be_explained(self, test_input, test_label):\n",
    "        # source : https://www.tensorflow.org/tutorials/interpretability/integrated_gradients\n",
    "\n",
    "        # selecting a sample to explain \n",
    "        #test_sample_index=np.random.randint(3000)\n",
    "        #test_input_temp = x_test[test_sample_index]\n",
    "        #test_input_temp_norm = test_input_temp/255.0\n",
    "\n",
    "        #print('image pixel values ', test_input_temp_norm)\n",
    "        #test_label = y_test[test_sample_index]\n",
    "        #test_label_string = cifar_class_map.get(np.argmax(test_label))\n",
    "        #ground_truth = np.argmax(test_label)\n",
    "\n",
    "        # show the selected sample\n",
    "        #plt.subplot(121)\n",
    "        #plt.imshow(test_input)\n",
    "        #plt.title('GT : '+cifar_class_map.get(np.argmax(test_label))+' ['+str(np.argmax(test_label))+']')\n",
    "      \n",
    "        #test_input = tf.expand_dims(test_input_temp, axis=0)\n",
    "\n",
    "        # plot the baseline\n",
    "        baseline = np.zeros(shape=(32,32,3))  # shape similar to that of input \n",
    "\n",
    "        #plt.subplot(122)\n",
    "        #plt.imshow(baseline)\n",
    "        #plt.title(\"Baseline\")\n",
    "        #plt.axis('off')\n",
    "        #plt.show()\n",
    "        return baseline\n",
    "\n",
    "\n",
    "    def generating_interpolated_images(self, baseline, image, m_steps):\n",
    "        # generating interpolated images\n",
    "        m_steps=m_steps\n",
    "        alphas = np.linspace(start=0.0, stop=1.0, num=m_steps+1) # Generate m_steps intervals for integral_approximation() below.\n",
    "        alphas_x = alphas[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "\n",
    "        baseline_x = np.expand_dims(baseline, axis=0)\n",
    "        baseline_x = np.ndarray.astype(baseline_x, dtype=np.float32)\n",
    "\n",
    "        input_x = np.expand_dims(image, axis=0)\n",
    "        input_x = np.ndarray.astype(input_x, dtype=np.float32)\n",
    "\n",
    "        delta = input_x - baseline_x\n",
    "        images = baseline_x +  alphas_x * delta\n",
    "        return alphas, images\n",
    "\n",
    "\n",
    "    def visualize_interpolated_images(self, alphas, images):\n",
    "        #convert numpy images to tensor\n",
    "        #interpolated_images_tensor = tf.convert_to_tensor(interpolated_images)\n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        i = 0\n",
    "        print('alphas ', alphas[0::10])\n",
    "        for alpha, image in zip(alphas[0::10], interpolated_images[0::10]):\n",
    "            i += 1\n",
    "            plt.subplot(1, len(alphas[0::10]), i)\n",
    "            plt.title(f'alpha: {alpha:.1f}')\n",
    "            #image_int = tf.cast(image, tf.int32)\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def compute_gradients(self, images, ground_truth, stoch_model):\n",
    "        model = tf.keras.models.Model(inputs=[stoch_model.input],\n",
    "                                      outputs=[stoch_model.output])    #this step done in order to avoid the 'IteratorGetNext' error \n",
    "        #images_adj = tf.squeeze(images, axis=0)  # images have a shape of (1, 51, 32, 32, 3), but model accepts (None, 51, 51, 3) therefore need to squeeze it\n",
    "        images_adj = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(images_adj)\n",
    "            outputs = model(images_adj)  # this is a (51, 10) vector that contains the predictions for the intermediate images in the form of softmax output\n",
    "            class_prediction = tf.math.argmax(outputs, axis=1)\n",
    "            max_output_predictions = tf.math.reduce_max(outputs, axis=1)\n",
    "            desired_class_logit = outputs[:, np.argmax(ground_truth)]\n",
    "            #print(desired_class_logit.shape)\n",
    "            \n",
    " \n",
    "        #probs = tf.nn.softmax(logits, axis=-1)[:, target_class_idx]\n",
    "        #desired_class_logit = outputs[: ,class_prediction]\n",
    "        #gradients = tape.gradient(desired_class_logit, images_adj)\n",
    "        gradients = tape.gradient(max_output_predictions, images_adj)\n",
    "        #print('gradients shape ', gradients.shape)\n",
    "        #print('class prediction : ', class_prediction)\n",
    "        class_prediction_np = list(class_prediction.numpy())   # for the ease of handling the output predictions\n",
    "        #print('class prediction list : ', class_prediction_np)\n",
    "\n",
    "        class_prediction_for_a_single_batch = max(set(class_prediction_np), key=class_prediction_np.count)          # https://stackoverflow.com/questions/1518522/find-the-most-common-element-in-a-list\n",
    "        #print('gradients ', gradients)\n",
    "        #print('gradients shape : ', gradients.shape)\n",
    "        #return tape.gradient(probs, images)  #original example had shape of (51, 224, 224, 3)\n",
    "        return class_prediction_for_a_single_batch, gradients\n",
    "\n",
    "\n",
    "    def integral_approximation(self, gradients):\n",
    "        # riemann_trapezoidal\n",
    "        grads = (gradients[:-1] + gradients[1:]) / 2.0\n",
    "        integrated_gradients = np.mean(grads, axis=0)\n",
    "        return integrated_gradients\n",
    "\n",
    "\n",
    "    def one_batch(self, baseline, image, ground_truth, alpha_batch, model, m_steps):\n",
    "        # Generate interpolated inputs between baseline and input.\n",
    "        interpolated_path_input_batch = self.generating_interpolated_images(baseline=baseline,\n",
    "                                                                            image=image,\n",
    "                                                                            m_steps=m_steps)[-1]    # since this function returns 4 arguments and here only the 4th is useful\n",
    "                                                                                                   # this will return (1, 51, 32, 32, 3) \n",
    "        # Compute gradients between model outputs and interpolated inputs.\n",
    "        class_prediction, gradient_batch = self.compute_gradients(images=interpolated_path_input_batch, \n",
    "                                                                  ground_truth=ground_truth, \n",
    "                                                                  stoch_model=model)\n",
    "                                        \n",
    "        return class_prediction, gradient_batch\n",
    "\n",
    "    \n",
    "    def integrated_gradients(self, \n",
    "                             baseline,\n",
    "                             image,\n",
    "                             ground_truth,\n",
    "                             m_steps=None,\n",
    "                             batch_size=None,\n",
    "                             model=None):\n",
    "        \n",
    "        # Generate alphas.\n",
    "        alphas = np.linspace(start=0.0, stop=1.0, num=m_steps+1)\n",
    "\n",
    "        # Collect gradients.    \n",
    "        gradient_batches = []\n",
    "        prediction_output_for_batches = [] \n",
    "        \n",
    "\n",
    "        # Iterate alphas range and batch computation for speed, memory efficiency, and scaling to larger m_steps.\n",
    "        for alpha in range(0, len(alphas), batch_size):\n",
    "            from_ = alpha\n",
    "            to = np.minimum(from_ + batch_size, len(alphas))\n",
    "            alpha_batch = alphas[from_:to]\n",
    "            #print('alpha batches :', alpha_batch)\n",
    "\n",
    "            #print('image to be explained \\n ', image)\n",
    "\n",
    "            prediction, gradient_batch = self.one_batch(baseline, image, ground_truth, alpha_batch, model, m_steps)\n",
    "            gradient_batches.append(gradient_batch)\n",
    "            prediction_output_for_batches.append(prediction)\n",
    "\n",
    "        # Concatenate path gradients together row-wise into single tensor.\n",
    "        total_gradients = np.concatenate(gradient_batches, axis=0)\n",
    "        #print('total gradients shape : ', total_gradients.shape)\n",
    "\n",
    "        # Integral approximation through averaging gradients.\n",
    "        avg_gradients = self.integral_approximation(gradients=total_gradients)\n",
    "        #print('avg gradients shape : ', avg_gradients.shape)\n",
    "\n",
    "        # Scale integrated gradients with respect to input.\n",
    "        integrated_gradients = (image - baseline) * avg_gradients\n",
    "\n",
    "        # identify the majority prediction in individual batch \n",
    "\n",
    "        #print('prediction_output_for_batches ', prediction_output_for_batches)\n",
    "\n",
    "        return prediction_output_for_batches, integrated_gradients\n",
    "\n",
    "\n",
    "    def plot_img_attributions(self, \n",
    "                              baseline,\n",
    "                              image,\n",
    "                              ground_truth,\n",
    "                              m_steps=None,\n",
    "                              model=None):\n",
    "\n",
    "        prediction_majority_per_batch, attributions = self.integrated_gradients(baseline=baseline,\n",
    "                                                 image=image,\n",
    "                                                 ground_truth=ground_truth,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 m_steps=m_steps,\n",
    "                                                 model=model)\n",
    "        \n",
    "        #print('prediction majority per batch : ', prediction_majority_per_batch)\n",
    "        prediction_majority_per_batch = [int(i[0]) if i.shape!=() else int(i) for i in prediction_majority_per_batch]\n",
    "        #print('prediction majority per batch : ', prediction_majority_per_batch)\n",
    "\n",
    "        \n",
    "        prediction = max(set(prediction_majority_per_batch), key=prediction_majority_per_batch.count)\n",
    "\n",
    "        #print('attributions shape : ', attributions.shape)\n",
    "        #print('baseline shape : ', baseline.shape)\n",
    "        #attributions = tf.squeeze(attributions, axis=0)\n",
    "        #baseline = tf.squeeze(baseline, axis=0)\n",
    "\n",
    "\n",
    "        #print('raw attributions : ', attributions)\n",
    "        #print('attributions shape : ', attributions.shape)\n",
    "        #print('baseline shape : ', baseline.shape)\n",
    "        #print('image shape : ', image.shape)\n",
    "        #print('prediction_majority_per_batch : ', prediction_majority_per_batch)\n",
    "\n",
    "        #attributions_np = attributions.numpy()\n",
    "\n",
    "        attributions_viz = np.dstack((attributions[:, :, 0],\n",
    "                                      attributions[:, :, 1],   # https://towardsdatascience.com/how-to-visually-explain-any-cnn-based-models-80e0975ce57\n",
    "                                      attributions[:, :, 2],\n",
    "                    ))      \n",
    "\n",
    "        #computing magnitude across channels \n",
    "        attributions_viz = np.sqrt(attributions_viz[:, :, 0]**2 + attributions_viz[:, :, 1]**2 + attributions_viz[:, :, 2]**2)\n",
    "        #print('attributions_viz ', attributions_viz)\n",
    "        #print('attributions_viz shape : ', attributions_viz.shape)\n",
    "\n",
    "        max_attributions_viz = np.max(attributions_viz)   # extracting the maximum value of heatmap generated by a individual model comprising the ensemble\n",
    "       \n",
    "        \n",
    "        normalized_attributions_viz = attributions_viz / max_attributions_viz  # normalizing individual heatmaps as per corresponding maximum value\n",
    "\n",
    "        #print('attributions_viz normalized ', normalized_attributions_viz)\n",
    "        \n",
    "        #print('maximum value in normalized heatmap number : ', i, ' ', np.max(normalized_attributions_viz)) \n",
    "\n",
    "        #print('maximum value in unnormalized heatmap number : ', i, ' ', np.max(attributions_viz))   # printing the maximum value before normalizing\n",
    "        \n",
    "\n",
    "        #ORIGINAL CODE \n",
    "        # Sum of the attributions across color channels for visualization.\n",
    "        # The attribution mask shape is a grayscale image with height and width\n",
    "        # equal to the original image.\n",
    "        #attribution_mask = tf.reduce_sum(tf.math.abs(attributions), axis=-1)\n",
    "        #attribution_mask_norm = attribution_mask/tf.math.reduce_max(attribution_mask)\n",
    "        print('GT : ', np.argmax(ground_truth))\n",
    "        print('prediction : ', prediction)\n",
    "         \n",
    "        return baseline, image, prediction, attributions_viz, normalized_attributions_viz\n",
    "    ''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the image to be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Analysis of the test input \n",
    "test_sample_index = np.random.randint(3000)\n",
    "#test_sample_index = 2813 #1891\n",
    "print('Random index ', test_sample_index)\n",
    "test_input_temp = x_test[test_sample_index]\n",
    "\n",
    "test_label = y_test[test_sample_index]\n",
    "test_label_string = cifar_class_map.get(np.argmax(test_label))\n",
    "ground_truth_prediction = np.argmax(test_label)\n",
    "\n",
    "test_input = np.expand_dims(test_input_temp, axis=0)\n",
    "#test_input = test_input/255.0\n",
    "print('test_input shape :', test_input.shape)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(test_input_temp)\n",
    "plt.title('GT : '+cifar_class_map.get(np.argmax(test_label))+' ['+str(np.argmax(test_label))+']')\n",
    "\n",
    "plt.subplot(122)\n",
    "image = (test_input_temp*255).astype(np.uint8)\n",
    "edge_image = cv2.Canny(image, 200, 200)\n",
    "plt.imshow(edge_image, cmap='Greys')\n",
    "plt.title('Edge image')\n",
    "plt.savefig(os.path.join(sanity_checks_output, 'data_randomization_original_image.png'))\n",
    "plt.savefig(os.path.join(sanity_checks_output, 'data_randomization_original_image.pdf'))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate GBP explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no data randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cmap='inferno'\n",
    "factor=0.01 # to stabilize the cv heatmap calculation \n",
    "number_of_explanations_single_image=10\n",
    "\n",
    "list_gbp_explanation_single_image=[]\n",
    "list_gbp_outputs_single_image=[]\n",
    "\n",
    "GBP_SSIM_mean = 0\n",
    "GBP_SSIM_std = 0\n",
    "\n",
    "trained_model_stochastic = StochasticClassifier(trained_model, \n",
    "                                                num_samples=num_samples)\n",
    "\n",
    "\n",
    "for i in range(number_of_explanations_single_image):\n",
    "    #print(test_label)\n",
    "    trained_model_hm, trained_model_pred = guided_bp(test_input, \n",
    "                                                     test_label,\n",
    "                                                     trained_model_stochastic,\n",
    "                                                     'dense_1')\n",
    "                                                 \n",
    "    trained_gbp_viz = np.dstack((trained_model_hm[:, :, 0],\n",
    "                                 trained_model_hm[:, :, 1],\n",
    "                                 trained_model_hm[:, :, 2]))\n",
    "\n",
    "    trained_gbp_viz = np.sqrt(trained_gbp_viz[:, :, 0]**2 + \n",
    "                              trained_gbp_viz[:, :, 1]**2 + \n",
    "                              trained_gbp_viz[:, :, 2]**2)\n",
    "    max_trained_gbp_viz = np.max(trained_gbp_viz)\n",
    "\n",
    "    if max_trained_gbp_viz == 0.0:\n",
    "        normalized_trained_gbp_viz = trained_gbp_viz / (max_trained_gbp_viz + 1e-5)\n",
    "        #print('max normalized heat map : ', np.max(normalized_gbp_viz))\n",
    "\n",
    "    else:\n",
    "        normalized_trained_gbp_viz = trained_gbp_viz / max_trained_gbp_viz\n",
    "    list_gbp_explanation_single_image.append(normalized_trained_gbp_viz)\n",
    "    list_gbp_outputs_single_image.append(trained_model_pred)\n",
    "    \n",
    "array_gbp_explanation_single_image=np.array(list_gbp_explanation_single_image)\n",
    "print(array_gbp_explanation_single_image.shape)\n",
    "trained_mean_gbp_explanation = np.mean(array_gbp_explanation_single_image, axis=0)\n",
    "trained_std_gbp_explanation = np.std(array_gbp_explanation_single_image, axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "random_model_stochastic = StochasticClassifier(random_model, \n",
    "                                               num_samples=num_samples)\n",
    "\n",
    "list_gbp_explanation_single_image=[]\n",
    "list_gbp_outputs_single_image=[]\n",
    "\n",
    "for i in range(number_of_explanations_single_image):\n",
    "    random_model_hm, random_model_pred = guided_bp(test_input, \n",
    "                                                    test_label,\n",
    "                                                    random_model_stochastic,\n",
    "                                                    'dense_1')\n",
    "                                                 \n",
    "    random_gbp_viz = np.dstack((random_model_hm[:, :, 0],\n",
    "                                random_model_hm[:, :, 1],\n",
    "                                random_model_hm[:, :, 2]))\n",
    "\n",
    "    random_gbp_viz = np.sqrt(random_gbp_viz[:, :, 0]**2 + \n",
    "                             random_gbp_viz[:, :, 1]**2 + \n",
    "                             random_gbp_viz[:, :, 2]**2)\n",
    "    max_random_gbp_viz = np.max(random_gbp_viz)\n",
    "\n",
    "    if max_random_gbp_viz == 0.0:\n",
    "        normalized_random_gbp_viz = random_gbp_viz / (max_random_gbp_viz + 1e-5)\n",
    "        #print('max normalized heat map : ', np.max(normalized_gbp_viz))\n",
    "\n",
    "    else:\n",
    "        normalized_random_gbp_viz = random_gbp_viz / max_random_gbp_viz\n",
    "    list_gbp_explanation_single_image.append(normalized_random_gbp_viz)\n",
    "    list_gbp_outputs_single_image.append(random_model_pred)\n",
    "    \n",
    "array_gbp_explanation_single_image=np.array(list_gbp_explanation_single_image)\n",
    "print(array_gbp_explanation_single_image.shape)\n",
    "random_mean_gbp_explanation = np.mean(array_gbp_explanation_single_image, axis=0)\n",
    "random_std_gbp_explanation = np.std(array_gbp_explanation_single_image, axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize GBP explanation for proper and randomized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "plt.title(f'input image \\nGT : {cifar_class_map.get(np.argmax(test_label))}')  \n",
    "plt.show()\n",
    "\n",
    "alpha_hm = 1\n",
    "alpha_hm = 0.8\n",
    "\n",
    "cv_trained_gbp_explanation = (trained_std_gbp_explanation + factor)/(trained_mean_gbp_explanation + factor)\n",
    "cv_random_gbp_explanation = (random_std_gbp_explanation + factor)/(random_mean_gbp_explanation + factor)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(321)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(trained_mean_gbp_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'GBP mean heatmap \\n (no data randomization)')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(trained_std_gbp_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'GBP std heatmap \\n (no data randomization)')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(random_mean_gbp_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'GBP mean heatmap \\n(data randomization)')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(random_std_gbp_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'GBP std heatmap \\n (data randomization)')\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(cv_trained_gbp_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'GBP coefficient of variation heatmap for\\n (no data randomization)')\n",
    "\n",
    "plt.subplot(326)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(cv_random_gbp_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'GBP coefficient of variation heatmap for\\n (data randomization)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(sanity_checks_output, 'data_randomization_GBP.png'))\n",
    "plt.savefig(os.path.join(sanity_checks_output, 'data_randomization_GBP.pdf'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "GBP_SSIM_mean = tf.image.ssim(np.expand_dims(trained_mean_gbp_explanation, axis=-1), \n",
    "                         np.expand_dims(random_mean_gbp_explanation, axis=-1), \n",
    "                         max_val=1.0, \n",
    "                         filter_size=11,\n",
    "                         filter_sigma=1.5, \n",
    "                         k1=0.01, \n",
    "                         k2=0.03)\n",
    "GBP_SSIM_std = tf.image.ssim(np.expand_dims(trained_std_gbp_explanation, axis=-1), \n",
    "                         np.expand_dims(random_std_gbp_explanation, axis=-1),\n",
    "                         max_val=1.0, \n",
    "                         filter_size=11,\n",
    "                         filter_sigma=1.5, \n",
    "                         k1=0.01, \n",
    "                         k2=0.03)\n",
    "\n",
    "print(f'GBP_SSIM_mean : {GBP_SSIM_mean}')\n",
    "print(f'GBP_SSIM_std : {GBP_SSIM_std}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate IG explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no data randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''list_ig_explanation_single_image=[]\n",
    "list_ig_outputs_single_image=[]\n",
    "list_ig_desired_class_score=[]\n",
    "\n",
    "IG_SSIM_mean = 0\n",
    "IG_SSIM_std = 0\n",
    "\n",
    "m_steps=50\n",
    "batch_size=26\n",
    "\n",
    "ig_explainer = IntegratedGradient() \n",
    "\n",
    "for i in range(number_of_explanations_single_image):\n",
    "    baseline = ig_explainer.visualize_input_to_be_explained(test_input[0, :, :, :], \n",
    "                                                            test_label)\n",
    "            \n",
    "    alphas, interpolated_images = ig_explainer.generating_interpolated_images(baseline=baseline,\n",
    "                                                                              image=test_input[0, :, :, :], \n",
    "                                                                              m_steps=m_steps)\n",
    "            \n",
    "    baseline_im, image_to_be_explained, prediction, ig_explanation, ig_explanation_normalized=ig_explainer.plot_img_attributions(baseline=baseline,\n",
    "                                                                                                                            image=test_input[0, :, :, :],\n",
    "                                                                                                                            ground_truth=test_label,\n",
    "                                                                                                                            m_steps=m_steps,\n",
    "                                                                                                                            model=trained_model_stochastic)\n",
    "                                                                                                                                                                     \n",
    "                \n",
    "\n",
    "    list_ig_explanation_single_image.append(ig_explanation_normalized)\n",
    "    list_ig_outputs_single_image.append(prediction)\n",
    "    \n",
    "array_ig_explanation_single_image=np.array(list_ig_explanation_single_image)\n",
    "print(array_ig_explanation_single_image.shape)\n",
    "trained_mean_ig_explanation = np.mean(array_ig_explanation_single_image, axis=0)\n",
    "trained_std_ig_explanation = np.std(array_ig_explanation_single_image, axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "list_ig_explanation_single_image=[]\n",
    "list_ig_outputs_single_image=[]\n",
    "\n",
    "IG_SSIM_mean = 0\n",
    "IG_SSIM_std = 0\n",
    "\n",
    "m_steps=50\n",
    "batch_size=26\n",
    "\n",
    "ig_explainer = IntegratedGradient() \n",
    "\n",
    "\n",
    "for i in range(number_of_explanations_single_image):\n",
    "    baseline = ig_explainer.visualize_input_to_be_explained(test_input[0, :, :, :], \n",
    "                                                            test_label)\n",
    "            \n",
    "    alphas, interpolated_images = ig_explainer.generating_interpolated_images(baseline=baseline,\n",
    "                                                                              image=test_input[0, :, :, :], \n",
    "                                                                              m_steps=m_steps)\n",
    "            \n",
    "    baseline_im, image_to_be_explained, prediction, ig_explanation, ig_explanation_normalized=ig_explainer.plot_img_attributions(baseline=baseline,\n",
    "                                                                                                                            image=test_input[0, :, :, :],\n",
    "                                                                                                                            ground_truth=test_label,\n",
    "                                                                                                                            m_steps=m_steps,\n",
    "                                                                                                                            model=random_model_stochastic)\n",
    "                                                                                                                                                                     \n",
    "                \n",
    "\n",
    "    list_ig_explanation_single_image.append(ig_explanation_normalized)\n",
    "    list_ig_outputs_single_image.append(prediction)\n",
    "    \n",
    "array_ig_explanation_single_image=np.array(list_ig_explanation_single_image)\n",
    "print(array_ig_explanation_single_image.shape)\n",
    "random_mean_ig_explanation = np.mean(array_ig_explanation_single_image, axis=0)\n",
    "random_std_ig_explanation = np.std(array_ig_explanation_single_image, axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize IG explanation for proper and randomized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "plt.title(f'input image \\nGT : {cifar_class_map.get(np.argmax(test_label))}')  \n",
    "plt.show()\n",
    "\n",
    "alpha_hm = 1\n",
    "alpha_hm = 0.8\n",
    "\n",
    "cv_trained_ig_explanation = (trained_std_ig_explanation + factor)/(trained_mean_ig_explanation + factor)\n",
    "cv_random_ig_explanation = (random_std_ig_explanation + factor)/(random_mean_ig_explanation + factor)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(321)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(trained_mean_ig_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'IG mean heatmap \\n (no data randomization)')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(trained_std_ig_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'IG std heatmap for\\n (no data randomization)')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(random_mean_ig_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'IG mean heatmap for\\n(data randomization)')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(random_std_ig_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'IG std heatmap for\\n (data randomization)')\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(cv_trained_ig_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'IG coefficient of variation heatmap for\\n (no data randomization)')\n",
    "\n",
    "plt.subplot(326)\n",
    "plt.imshow(test_input[0, :, :, :])\n",
    "hm=plt.imshow(cv_random_ig_explanation, cmap=cmap, alpha=alpha_hm)\n",
    "plt.colorbar(hm)\n",
    "plt.title(f'IG coefficient of variation heatmap for\\n (data randomization)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(sanity_checks_output, 'data_randomization_IG.png'))\n",
    "plt.savefig(os.path.join(sanity_checks_output, 'data_randomization_IG.pdf'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "IG_SSIM_mean = tf.image.ssim(np.expand_dims(trained_mean_ig_explanation, axis=-1), \n",
    "                             np.expand_dims(random_mean_ig_explanation, axis=-1), \n",
    "                             max_val=1.0, \n",
    "                             filter_size=11,\n",
    "                             filter_sigma=1.5, \n",
    "                             k1=0.01, \n",
    "                             k2=0.03)\n",
    "\n",
    "IG_SSIM_std = tf.image.ssim(np.expand_dims(trained_std_ig_explanation, axis=-1), \n",
    "                            np.expand_dims(random_std_ig_explanation, axis=-1),\n",
    "                            max_val=1.0, \n",
    "                            filter_size=11,\n",
    "                            filter_sigma=1.5, \n",
    "                            k1=0.01, \n",
    "                            k2=0.03)\n",
    "print(f'GBP_SSIM_mean : {GBP_SSIM_mean}')\n",
    "print(f'GBP_SSIM_std : {GBP_SSIM_std}')\n",
    "print(f'IG_SSIM_mean : {IG_SSIM_mean}')\n",
    "print(f'IG_SSIM_std : {IG_SSIM_std}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices : 2813"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
