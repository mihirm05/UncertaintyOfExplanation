{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GBP_CHD_deep_ensemble_training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPS6HE4nnRw1bthZGofBlgh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Mount the drive"],"metadata":{"id":"5AXmJYQvlxwA"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNNMAsCplpCx","executionInfo":{"status":"ok","timestamp":1646513588880,"user_tz":-60,"elapsed":2282,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"499c8aef-702f-488c-ce46-daa24787ea2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# mount the drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## Removing previous trained models"],"metadata":{"id":"dSssFUl2l3Z3"}},{"cell_type":"code","source":["import os\n","\n","dir_name = '/content/drive/MyDrive/MasterThesis/CaliforniaHousingDatasetTests/GBP_explanation/deep_ensemble/'\n","test = os.listdir(dir_name)\n","\n","for item in test:\n","    if item.endswith('.h5'):\n","        print(f'deleting {item}')\n","        os.remove(os.path.join(dir_name, item))"],"metadata":{"id":"0dvEYk2-l08v","executionInfo":{"status":"ok","timestamp":1646513588882,"user_tz":-60,"elapsed":17,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"OnR8zW90mCEF"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np \n","import matplotlib.pyplot as plt \n","import tensorflow as tf\n","\n","tf.compat.v1.disable_eager_execution()"],"metadata":{"id":"lvisT5UAmC6K","executionInfo":{"status":"ok","timestamp":1646513592632,"user_tz":-60,"elapsed":3761,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Installing keras uncertainty"],"metadata":{"id":"xhBv7T5rmFPX"}},{"cell_type":"code","source":["# keras_uncertainty imports \n","# clone and install this library \n","\n","!git clone https://github.com/mvaldenegro/keras-uncertainty.git\n","!pip install --user git+https://github.com/mvaldenegro/keras-uncertainty.git\n","\n","%cd keras-uncertainty"],"metadata":{"id":"DKg3SQrvmIEB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646513600064,"user_tz":-60,"elapsed":7458,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"a3580a28-2b36-4d2d-d96d-3fd7b1b3acba"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'keras-uncertainty' already exists and is not an empty directory.\n","Collecting git+https://github.com/mvaldenegro/keras-uncertainty.git\n","  Cloning https://github.com/mvaldenegro/keras-uncertainty.git to /tmp/pip-req-build-cypgv_xh\n","  Running command git clone -q https://github.com/mvaldenegro/keras-uncertainty.git /tmp/pip-req-build-cypgv_xh\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from Keras-Uncertainty==0.0.1) (2.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from Keras-Uncertainty==0.0.1) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Keras-Uncertainty==0.0.1) (4.63.0)\n","/content/keras-uncertainty\n"]}]},{"cell_type":"markdown","source":["## Keras uncertainty specific imports"],"metadata":{"id":"YH_mlRtXmL2T"}},{"cell_type":"code","source":["import numpy as np \n","import tensorflow as tf \n","from tensorflow.keras.models import load_model\n","import random\n","import pandas as pd \n","import os \n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import math \n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Input\n","\n","import keras_uncertainty\n","#from keras_uncertainty.models import StochasticRegressor, TwoHeadStochasticRegressor\n","#from keras_uncertainty.models.DeepEnsembleClassifier import DeepEnsemble\n","\n","#from keras_uncertainty.layers import DropConnectDense, VariationalDense, FlipoutDense, StochasticDropout\n","from keras_uncertainty.metrics import gaussian_interval_score\n","from keras_uncertainty.losses import regression_gaussian_nll_loss, regression_gaussian_beta_nll_loss\n","import matplotlib.pyplot as plt\n","\n","np.set_printoptions(suppress=True) \n","\n","import tensorflow as tf\n","#tf.compat.v1.disable_eager_execution()\n","\n","# 28022022 the code works without disabling the eager execution (dont know why) \n","\n","# if eager execution is not disabled following error occurs:\n","# TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), \n","#dtype=tf.float32, name=None), name='Placeholder:0', description=\"created \n","#by layer 'tf.cast_4'\"), an intermediate Keras symbolic input/output, \n","#to a TF API that does not allow registering custom dispatchers, \n","#such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. \n","#Keras Functional model construction only supports TF API calls that \n","#*do* support dispatching, such as `tf.math.add` or `tf.reshape`. \n","#Other APIs cannot be called directly on symbolic Kerasinputs/outputs. \n","#You can work around this limitation by putting the operation in a custom \n","#Keras layer `call` and calling that layer on this symbolic input/output."],"metadata":{"id":"isq5KIlUmPI4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646513600066,"user_tz":-60,"elapsed":65,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"b03f5bc1-ce44-484b-9441-64d62389c630"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Keras Uncertainty will use standalone Keras backend"]}]},{"cell_type":"markdown","source":["## Load data"],"metadata":{"id":"0ub_YOCwmtdm"}},{"cell_type":"code","source":["# load the california housing data from csv\n","train_file = '/content/sample_data/california_housing_train.csv'\n","test_file = '/content/sample_data/california_housing_test.csv'\n","\n","train_combined = pd.read_csv(train_file)\n","test = pd.read_csv(test_file)\n","\n","# split the data in validation and test (from test.csv)\n","train, val = train_test_split(train_combined, test_size=0.25)\n","\n","feature_names = list(train_combined.columns)\n","print(feature_names)\n","\n","# assign the target variable\n","target = 'median_house_value'\n","\n","# extract the target label in all sets\n","train_labels_df= train[target]\n","val_labels_df = val[target]\n","test_labels_df = test[target]\n","\n","# extract the data from all sets \n","train_data_df = train.drop(columns=target, axis=1)\n","val_data_df = val.drop(columns=target, axis=1)\n","test_data_df = test.drop(columns=target, axis=1)\n","\n","train_data_unnormalized = train_data_df.to_numpy()\n","train_labels_unnormalized = train_labels_df.to_numpy()\n","\n","val_data_unnormalized = val_data_df.to_numpy()\n","val_labels_unnormalized = val_labels_df.to_numpy()\n","\n","test_data_unnormalized = test_data_df.to_numpy()\n","test_labels_unnormalized = test_labels_df.to_numpy()\n","\n","# normalize the data using minmax \n","minmax = MinMaxScaler() \n","\n","train_data = minmax.fit_transform(train_data_unnormalized)\n","train_label_temp = np.expand_dims(train_labels_unnormalized, axis=1)\n","train_labels = minmax.fit_transform(train_label_temp)\n","\n","val_data = minmax.fit_transform(val_data_unnormalized)\n","val_label_temp = np.expand_dims(val_labels_unnormalized, axis=1)\n","val_labels = minmax.fit_transform(val_label_temp)\n","\n","test_data = minmax.fit_transform(test_data_unnormalized)\n","test_label_temp = np.expand_dims(test_labels_unnormalized, axis=1)\n","test_labels = minmax.fit_transform(test_label_temp)\n","\n","\n","print('Training data shape \\n', train_data.shape)\n","print('Training labels shape \\n', train_labels.shape)\n","#print('Training data \\n ', train_data)\n","#print('Training labels \\n ', train_labels)\n","\n","print('Validation data shape \\n ',val_data.shape)\n","print('Validation labels shape \\n ', val_labels.shape)\n","#print('Validation data \\n ', val_data)\n","#print('Validation labels \\n ', val_labels)\n","\n","print('Test data shape \\n ', test_data.shape)\n","print('Test labels shape \\n ', test_labels.shape)\n","#print('Test data \\n ', test_data)\n","#print('Test labels \\n ', test_labels)# load the california housing data from csv"],"metadata":{"id":"kBGxFhDbmugo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646513600444,"user_tz":-60,"elapsed":432,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"a0d007bb-0162-4090-9f53-b844dd950d10"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n","Training data shape \n"," (12750, 8)\n","Training labels shape \n"," (12750, 1)\n","Validation data shape \n","  (4250, 8)\n","Validation labels shape \n","  (4250, 1)\n","Test data shape \n","  (3000, 8)\n","Test labels shape \n","  (3000, 1)\n"]}]},{"cell_type":"markdown","source":["## DeepEnsemble and DeepEnsembleRegressor"],"metadata":{"id":"5_ZQaVcVm3In"}},{"cell_type":"code","source":["import numpy as np\n","\n","import os\n","import yaml\n","import keras_uncertainty.backend as K\n","from pydoc import locate\n","\n","class AdversarialExampleGenerator:\n","    pass\n","\n","METADATA_FILENAME = \"metadata.yml\"\n","\n","class DeepEnsemble:\n","    def __init__(self, model_fn=None, num_estimators=None, models=None, needs_test_estimators=False):\n","        self.needs_test_estimators = needs_test_estimators\n","\n","        if models is None:\n","            assert model_fn is not None and num_estimators is not None\n","            assert num_estimators > 0\n","            \n","            self.num_estimators = num_estimators\n","            self.train_estimators = [None] * num_estimators \n","            self.test_estimators = [None] * num_estimators\n","\n","            for i in range(self.num_estimators):\n","                if self.needs_test_estimators:\n","                    estimators = model_fn()\n","\n","                    if type(estimators) is not tuple:\n","                        raise ValueError(\"model_fn should return a tuple\")\n","\n","                    if len(estimators) != 2:\n","                        raise ValueError(\"model_fn returned a tuple of unexpected size ({} vs 2)\".format(len(estimators)))\n","\n","                    train_est, test_est = estimators\n","                    self.train_estimators[i] = train_est\n","                    self.test_estimators[i] = test_est\n","                else:\n","                    est = model_fn()\n","                    self.train_estimators[i] = est\n","                    self.test_estimators[i] = est\n","\n","        else:\n","            assert model_fn is None and num_estimators is None\n","\n","            self.train_estimators = models\n","            self.test_estimators = models\n","\n","            self.num_estimators = len(models)\n","\n","    def save(self, folder, filename_pattern=\"model-ensemble-{}.hdf5\"):\n","        \"\"\"\n","            Save a Deep Ensemble into a folder, using individual HDF5 files for each ensemble member.\n","            This allows for easily loading individual ensembles. Metadata is saved to allow loading of the whole ensemble.\n","        \"\"\"\n","\n","        if not os.path.exists(folder):\n","            os.makedirs(folder)\n","\n","        model_metadata = {}\n","\n","        for i in range(self.num_estimators):\n","            filename = os.path.join(folder, filename_pattern.format(i))\n","            self.test_estimators[i].save(filename)\n","\n","            print(\"Saved estimator {} to {}\".format(i, filename))\n","\n","            model_metadata[i] = filename_pattern.format(i)\n","\n","        metadata = {\"models\": model_metadata, \"class\": self.__module__}\n","\n","        with open(os.path.join(folder, METADATA_FILENAME), 'w') as outfile:\n","            yaml.dump(metadata, outfile)\n","            \n","\n","    @staticmethod\n","    def load(folder):\n","        \"\"\"\n","            Load a Deep Ensemble model from a folder containing individual HDF5 files.\n","        \"\"\"\n","        metadata = {}\n","\n","        with open(os.path.join(folder, METADATA_FILENAME)) as infile:\n","            metadata = yaml.full_load(infile)\n","\n","        models = []\n","\n","        for _, filename in metadata[\"models\"].items():\n","            models.append(keras.models.load_model(os.path.join(folder, filename)))\n","\n","        clazz = locate(metadata[\"class\"])\n","\n","        return clazz(models=models)  \n","\n","class DeepEnsembleRegressor(DeepEnsemble):\n","    \"\"\"\n","        Implementation of a Deep Ensemble for regression.\n","        Uses two models, one for training and another for inference/testing. The user has to provide a model function that returns\n","        the train and test models, and use the provided deep_ensemble_nll_loss for training.\n","    \"\"\"\n","    def __init__(self, model_fn=None, num_estimators=None, models=None):\n","        \"\"\"\n","            Builds a Deep Ensemble given a function to make model instances, and the number of estimators.\n","            For training it uses a model that only outputs the mean, while the loss uses both the mean and variance produced by the model.\n","            For testing, a model that shares weights with the training model is used, but the testing model outputs both mean and variance. The final\n","            prediction is made with a mixture of gaussians, where each gaussian is one trained model instance.\n","        \"\"\"\n","        super().__init__(model_fn=model_fn, num_estimators=num_estimators, models=models,\n","                         needs_test_estimators=True)\n","\n","    def fit(self, X, y, epochs=10, batch_size=32, **kwargs):\n","        \"\"\"\n","            Fits the Deep Ensemble, each estimator is fit independently on the same data.\n","        \"\"\"\n","\n","        for i in range(self.num_estimators):\n","            self.train_estimators[i].fit(X, y, epochs=epochs, batch_size=batch_size, **kwargs)\n","    \n","    def fit_generator(self, generator, epochs=10, **kwargs):\n","        \"\"\"\n","            Fits the Deep Ensemble, each estimator is fit independently on the same data.\n","        \"\"\"\n","\n","        for i in range(self.num_estimators):\n","            self.train_estimators[i].fit_generator(generator, epochs=epochs, **kwargs)\n","            \n","\n","    def predict_output(self, X, batch_size=32, output_scaler=None, num_ensembles=None, disentangle_uncertainty=False, num_samples=10, **kwargs):\n","        \"\"\"\n","            Makes a prediction. Predictions from each estimator are used to build a gaussian mixture and its mean and standard deviation returned.\n","        \"\"\"\n","        \n","        means = []\n","        variances = []\n","\n","        if num_ensembles is None:\n","            estimators = self.test_estimators\n","        else:\n","            estimators = self.test_estimators[:num_ensembles]\n","\n","        if \"verbose\" not in kwargs:\n","            kwargs[\"verbose\"] = 0\n","\n","        for estimator in estimators:\n","            mean, var  = estimator.predict(X, batch_size=batch_size, **kwargs)\n","\n","            if output_scaler is not None:\n","                mean = output_scaler.inverse_transform(mean)\n","\n","                # This should work but not sure if its 100% correct\n","                # Its not clear how to do inverse scaling of the variance\n","                sqrt_var = np.sqrt(var)\n","                var = output_scaler.inverse_transform(sqrt_var)\n","                var = np.square(var)\n","\n","            means.append(mean)\n","            variances.append(var)\n","\n","        means = np.array(means)\n","        variances = np.array(variances)\n","        \n","        mixture_mean = np.mean(means, axis=0)\n","        mixture_var  = np.mean(variances + np.square(means), axis=0) - np.square(mixture_mean)\n","        mixture_var[mixture_var < 0.0] = 0.0\n","                \n","        if disentangle_uncertainty:\n","            epi_var = np.var(means, axis=0)\n","            ale_var = np.mean(variances, axis=0)\n","\n","            return mixture_mean, np.sqrt(ale_var), np.sqrt(epi_var)\n","\n","        sample = np.random.normal(mixture_mean, np.sqrt(mixture_var), num_samples)\n","\n","        return sample, mixture_mean, np.sqrt(mixture_var)\n","\n","    def predict_generator(self, generator, steps=None, num_ensembles=None, **kwargs):\n","        \"\"\"\n","            Makes a prediction. Predictions from each estimator are used to build a gaussian mixture and its mean and standard deviation returned.\n","        \"\"\"\n","        \n","        means = []\n","        variances = []\n","\n","        if num_ensembles is None:\n","            estimators = self.test_estimators\n","        else:\n","            estimators = self.test_estimators[:num_ensembles]\n","\n","        for estimator in estimators:\n","            mean, var  = estimator.predict_generator(generator, steps=steps, **kwargs)\n","            means.append(mean)\n","            variances.append(var)\n","\n","        means = np.array(means)\n","        variances = np.array(variances)\n","        \n","        mixture_mean = np.mean(means, axis=0)\n","        mixture_var  = np.mean(variances + np.square(means), axis=0) - np.square(mixture_mean)\n","        mixture_var[mixture_var < 0.0] = 0.0\n","                \n","        return mixture_mean, np.sqrt(mixture_var)"],"metadata":{"id":"Ktrn7nJsm41P","executionInfo":{"status":"ok","timestamp":1646513600834,"user_tz":-60,"elapsed":399,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Define Ensemble model"],"metadata":{"id":"I8ZtUiDaw5yv"}},{"cell_type":"code","source":["# ENSEMBLE MODEL\n","def train_ensemble_model(x_train, y_train, x_val, y_val, x_test, epochs, num_estimators):\n","    #obtained from hyperparameter optimization\n","    def model_fn():\n","        K.clear_session()\n","        inp = Input(shape=(8,))\n","        x = Dense(32, activation='relu')(inp)\n","        x = Dense(32, activation='relu')(x)\n","        mean = Dense(1, activation='linear')(x)\n","        var = Dense(1, activation='softplus')(x)\n","\n","        train_model = Model(inp, mean)\n","        pred_model = Model(inp, [mean, var])\n","        train_model.compile(loss=regression_gaussian_nll_loss(var), optimizer='adam', metrics=['mae'])\n","\n","        return train_model, pred_model \n","\n","    ens_model = DeepEnsembleRegressor(model_fn, num_estimators=num_estimators)\n","\n","    # train a model with stochasticdropout() layer\n","    history = ens_model.fit(x_train, y_train, validation_data=(x_val, y_val), verbose=2, epochs=epochs)\n","    #saving the model that has custom layers\n","    ens_model.save('/content/drive/MyDrive/MasterThesis/CaliforniaHousingDatasetTests/GBP_explanation/deep_ensemble/ensemble_model_epochs_'+str(epochs)+'_num_estimators_'+str(num_estimators)+'.h5') \n","    ens_model.summary()\n","\n","    # plotting the training and validation curves\n","    plt.plot(history.history['loss'], label='train loss')\n","    plt.plot(history.history['val_loss'], label='val loss')\n","    plt.legend()\n","    plt.grid()\n","    plt.xlabel('epochs')\n","    plt.ylabel('loss')\n","    plt.title('loss curves')\n","    plt.show()\n","\n","    plt.plot(history.history['mae'], label='train mae')\n","    plt.plot(history.history['val_mae'], label='val mae')\n","    plt.legend()\n","    plt.grid()\n","    plt.xlabel('epochs')\n","    plt.ylabel('mae')\n","    plt.title('mae curves')\n","    plt.show()\n","\n","    pred_samples, pred_mean, pred_std = ens_model.predict_output(x_test, num_samples=num_samples)\n","    print('pred_samples shape ', pred_samples.shape)\n","    print('pred_mean shape ', pred_mean.shape)\n","    print('pred_std shape ', pred_std.shape)\n","\n","    # return the predicted_samples, predicted_mean, predicted_std and model \n","    return pred_samples, pred_mean, pred_std, ens_model\n","\n","\n","epochs=3\n","num_estimators=5\n","\n","# running this command creates the stochastic dropout model, trains it and generates the predicted_samples, predicted_mean and predicted_std for the test set (all 3000 examples)\n","prediction_samples, prediction_mean, prediction_std, ensemble_model = train_ensemble_model(train_data, train_labels, val_data, val_labels, test_data, epochs=epochs, num_estimators=num_estimators)\n","print('prediction mean :\\n', prediction_mean)\n","print('prediction mean shape ', prediction_mean.shape)\n","print('prediction std :\\n', prediction_std)\n","print('prediction std shape ', prediction_std.shape)\n","print('prediction samples :\\n', prediction_samples)\n","print('prediction samples shape ', prediction_samples.shape)"],"metadata":{"id":"TpL32rHEw7tm","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1646513603249,"user_tz":-60,"elapsed":2420,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"ee3908e4-6c14-402d-d3c4-766fb3bf4dd2"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c506a4b4d49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# running this command creates the stochastic dropout model, trains it and generates the predicted_samples, predicted_mean and predicted_std for the test set (all 3000 examples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mprediction_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ensemble_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction mean :\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction mean shape '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-c506a4b4d49e>\u001b[0m in \u001b[0;36mtrain_ensemble_model\u001b[0;34m(x_train, y_train, x_val, y_val, x_test, epochs, num_estimators)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# train a model with stochasticdropout() layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mens_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m#saving the model that has custom layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mens_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/MasterThesis/CaliforniaHousingDatasetTests/GBP_explanation/deep_ensemble/ensemble_model_epochs_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_num_estimators_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-4a29170c94cd>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_estimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m   \u001b[0;31m# function we recompile the metrics based on the updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[0;31m# sample_weight_mode value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m   \u001b[0;31m# Prepare validation data. Hold references to the iterator and the input list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m    546\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistributed_training_utils_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2085\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2015\u001b[0m           \u001b[0;31m# Training updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m           updates = self.optimizer.get_updates(\n\u001b[0;32m-> 2017\u001b[0;31m               params=self._collected_trainable_weights, loss=self.total_loss)\n\u001b[0m\u001b[1;32m   2018\u001b[0m           \u001b[0;31m# Unconditional updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2019\u001b[0m           \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_updates_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     ])\n\u001b[0;32m--> 775\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_hyper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    673\u001b[0m           \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m           \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribution_strategy_context.get_replica_context().merge_call(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, apply_state, name)\u001b[0m\n\u001b[1;32m    715\u001b[0m               var.op.name):\n\u001b[1;32m    716\u001b[0m             update_op = distribution.extended.update(\n\u001b[0;32m--> 717\u001b[0;31m                 var, apply_grad_to_update_var, args=(grad,), group=False)\n\u001b[0m\u001b[1;32m    718\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_cross_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m               \u001b[0;31m# In cross-replica context, extended.update returns a list of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2628\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   2629\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m       return self._replica_ctx_update(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3701\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3702\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3703\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3707\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3709\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3710\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"apply_state\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"apply_state\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[0;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[1;32m    174\u001b[0m           \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m           use_locking=self._use_locking)\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0mvhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vhat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m           \u001b[0;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[0;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[1;32m   1447\u001b[0m                              \u001b[0mbeta2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m                              \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m                              use_nesterov=use_nesterov, name=name)\n\u001b[0m\u001b[1;32m   1450\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0mResourceApplyAdam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw_ops.ResourceApplyAdam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_raw_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_apply_adam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   6418\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graph\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6419\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6420\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6421\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6422\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   6353\u001b[0m     raise ValueError(\n\u001b[1;32m   6354\u001b[0m         \u001b[0;34m\"%s must be from the same graph as %s (graphs are %s and %s).\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6355\u001b[0;31m         (item, original_item, graph, original_graph))\n\u001b[0m\u001b[1;32m   6356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Tensor(\"training/Adam/dense/kernel/m:0\", shape=(), dtype=resource) must be from the same graph as Tensor(\"dense/kernel:0\", shape=(), dtype=resource) (graphs are <tensorflow.python.framework.ops.Graph object at 0x7f90aea37710> and <tensorflow.python.framework.ops.Graph object at 0x7f90cb915810>)."]}]},{"cell_type":"code","source":["# Analysis of the input \n","num_of_samples_to_be_explained = 1\n","start_index = np.random.randint(0, test_data.shape[0])\n","print('start_index : ', start_index)\n","\n","test_input = test_data[start_index:start_index+num_of_samples_to_be_explained]\n","print('test_input shape :', test_input.shape)\n","\n","test_input_adj = np.expand_dims(test_input, axis=-1)\n","print('test_input_adj shape :', test_input_adj.shape)\n","\n","pred_samples, pred_mean, pred_std = dropout_model.predict_output(test_data)\n","print(pred_samples.shape) \n","print(pred_mean.shape)\n","print(pred_std.shape)"],"metadata":{"id":"r6d5ArfR1M1A","executionInfo":{"status":"aborted","timestamp":1646513603248,"user_tz":-60,"elapsed":11,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to visualize the ground truth with the predicted value and (corridor of uncertainty)\n","\n","def plot(ground_truth, prediction_mean, prediction_std, path, indices_to_be_plotted):\n","    plt.figure(figsize=(30, 4))\n","    plt.plot(range(ground_truth.shape[0]),  ground_truth, color='k', label='ground truth', marker='o')\n","    plt.plot(range(ground_truth.shape[0]), prediction_mean, color='r', label='prediction', marker='o')\n","   \n","    y_pred_mean = prediction_mean.reshape((-1,))\n","    y_pred_std = prediction_std.reshape((-1,))\n","    y_pred_up_1 = y_pred_mean + y_pred_std\n","    y_pred_down_1 = y_pred_mean - y_pred_std\n","\n","    plt.fill_between(range(ground_truth.shape[0]), y_pred_down_1, y_pred_up_1, color=(0, 0, 0.9, 0.7), label='corridor of uncertainty ($\\pm$ 1 $\\sigma$) ', alpha=0.5)\n","    #plt.plot(range(ground_truth.shape[0]), y_pred_mean, '.', color=(0, 0.9, 0.0, 0.8), markersize=0.2, label='Mean')\n","\n","    #plt.set_title('{}\\nInterval Score: {:.2f}'.format(key, score))\n","    #plt.set_ylim([-20.0, 20.0])\n","\n","    #plt.axvline(x=-4.0, color='black', linestyle='dashed')\n","    #plt.axvline(x= 4.0, color='black', linestyle='dashed')\n","    #plt.get_xaxis().set_ticks([])\n","    #plt.get_yaxis().set_ticks([])    \n","\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","    plt.legend()\n","    plt.grid()\n","    plt.xticks(range(len(indices_to_be_plotted)), indices_to_be_plotted, rotation=45)\n","    plt.xlabel('Input sample #')\n","    plt.ylabel('Target Variable (normalized)')\n","    plt.title('Ground Truth and Dropout Model Prediction')\n","    plt.savefig(path)\n","    plt.show()\n","\n","start_index = np.random.randint(test_data.shape[0]-150)\n","random = range(start_index, start_index+100)\n","print('indices to be plotted \\n', random)\n","test_labels_plot= np.asarray([float(test_labels[i]) for i in random])\n","mean_dropout_plot = np.asarray([float(pred_mean[i]) for i in random])\n","std_dropout_plot = np.asarray([float(pred_std[i]) for i in random])\n","\n","plot(test_labels_plot, mean_dropout_plot, std_dropout_plot, '/content/drive/MyDrive/MasterThesis/CaliforniaHousingDatasetTests/GBP_explanation/dropout/output_plots/dropout_gt_vs_prediction.pdf', random)"],"metadata":{"id":"mzJbm9fv1P4m","executionInfo":{"status":"aborted","timestamp":1646513603249,"user_tz":-60,"elapsed":12,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":null,"outputs":[]}]}