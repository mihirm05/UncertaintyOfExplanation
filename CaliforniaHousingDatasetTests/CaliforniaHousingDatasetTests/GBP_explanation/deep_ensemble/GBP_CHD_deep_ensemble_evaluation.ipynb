{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GBP_CHD_deep_ensemble_evaluation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNzMa/OgB8dLuYWD5AQt+Ed"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Summary of the notebook"],"metadata":{"id":"K8uceMNrtHSA"}},{"cell_type":"markdown","source":["## Mount the drive "],"metadata":{"id":"uymzvZC-tEe6"}},{"cell_type":"code","source":["# mount the drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UL94iw_fIFZ6","executionInfo":{"status":"ok","timestamp":1648384816992,"user_tz":-120,"elapsed":2020,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"cefe3712-58ff-4b74-c392-09efed8d39e7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## Setting the correct path"],"metadata":{"id":"XuKwPfpeGO_B"}},{"cell_type":"code","source":["import os\n","\n","project_path = '/content/drive/MyDrive/MasterThesis'\n","task_name = 'CaliforniaHousingDatasetTests'\n","exp_name = 'GBP_explanation'\n","uncert_name = 'deep_ensemble'\n","\n","%cd /content/drive/MyDrive/MasterThesis/ \n","\n","print(os.getcwd())\n","path = project_path + '/' + task_name + '/' + exp_name + '/' + uncert_name + '/' # 'CaliforniaHousingDatasetTests/GBP_explanation/flipout/'\n","print(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rsNLIJzILeK","executionInfo":{"status":"ok","timestamp":1648384816994,"user_tz":-120,"elapsed":24,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"2dd97b99-2047-477c-a03f-615bb9140689"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MasterThesis\n","/content/drive/MyDrive/MasterThesis\n","/content/drive/MyDrive/MasterThesis/CaliforniaHousingDatasetTests/GBP_explanation/deep_ensemble/\n"]}]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"y4fsdZ-OGRXs"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np \n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","import os\n","import random \n","import math\n","\n","from keras.layers import *\n","from keras.models import Model, Sequential\n","import keras.backend as K\n","\n","tf.compat.v1.disable_eager_execution()\n","print(tf.__version__)\n","print(f'Eager execution enabled : {tf.executing_eagerly()}')\n","\n","import sys\n","sys.path.insert(0,'/content/drive/My Drive/MasterThesis/src/')\n","\n","from utils import *"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43X4vDxxJzBx","executionInfo":{"status":"ok","timestamp":1648384822167,"user_tz":-120,"elapsed":5186,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"7f0c38bc-ac6f-4b8e-afb4-d76d99c2312d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.0\n","Eager execution enabled : False\n"]}]},{"cell_type":"markdown","source":["## regression_nll_loss"],"metadata":{"id":"C9vie102J13X"}},{"cell_type":"code","source":["import tensorflow as tf \n","\n","def regression_gaussian_nll_loss(variance_tensor, epsilon=1e-8, variance_logits=False):\n","    \"\"\"\n","        Gaussian negative log-likelihood for regression, with variance estimated by the model.\n","        This function returns a keras regression loss, given a symbolic tensor for the sigma square output of the model.\n","        The training model should return the mean, while the testing/prediction model should return the mean and variance.\n","    \"\"\"\n","    def nll(y_true, y_pred):\n","        #if variance_logits:\n","        #    variance_tensor = K.exp(variance_tensor)\n","\n","        return 0.5 * tf.math.reduce_mean(tf.math.log(variance_tensor + epsilon) + tf.math.square(y_true - y_pred) / (variance_tensor + epsilon))\n","\n","    return nll"],"metadata":{"id":"2p7A1kdMJ4jq","executionInfo":{"status":"ok","timestamp":1648384822168,"user_tz":-120,"elapsed":40,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Load data"],"metadata":{"id":"T0OMozS1GU3r"}},{"cell_type":"code","source":["train_data, train_labels, val_data, val_labels, test_data, test_labels, feature_names = load_california_housing_data()\n","#print(train_labels[20])\n","#print(val_labels[30])\n","#print(test_labels[40])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BbMmb0IAK8Ra","executionInfo":{"status":"ok","timestamp":1648384822170,"user_tz":-120,"elapsed":38,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"d8d0d5ca-38ad-463a-ec42-7a1a68764676"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n","Training data shape \n"," (12750, 8)\n","Training labels shape \n"," (12750, 1)\n","Validation data shape \n","  (4250, 8)\n","Validation labels shape \n","  (4250, 1)\n","Test data shape \n","  (3000, 8)\n","Test labels shape \n","  (3000, 1)\n"]}]},{"cell_type":"markdown","source":["## Class definition"],"metadata":{"id":"OMTMKH5HGYMu"}},{"cell_type":"markdown","source":["### DeepEnsemble"],"metadata":{"id":"z5nbYWr8Gai4"}},{"cell_type":"code","source":["import keras\n","import numpy as np\n","import keras\n","\n","import os\n","import yaml\n","\n","from pydoc import locate\n"," \n","METADATA_FILENAME = \"metadata.yml\"\n","\n","class DeepEnsemble:\n","    def __init__(self, model_fn=None, num_estimators=None, models=None, needs_test_estimators=False):\n","        self.needs_test_estimators = needs_test_estimators\n","        self.model_fn = model_fn\n","        self.num_estimators = num_estimators\n","        self.models = models\n","\n","        if models is None:\n","            assert model_fn is not None and num_estimators is not None\n","            assert num_estimators > 0\n","            \n","            self.num_estimators = num_estimators\n","            self.train_estimators = [None] * num_estimators \n","            self.test_estimators = [None] * num_estimators\n","\n","            print('train_estimators ', self.train_estimators)\n","            print('test_estimators ', self.test_estimators)\n","            print('num_estimators ', self.num_estimators)\n","            \n","\n","            for i in range(self.num_estimators):\n","                if self.needs_test_estimators:\n","                    estimators = model_fn()\n","\n","                    if type(estimators) is not tuple:\n","                        raise ValueError(\"model_fn should return a tuple\")\n","\n","                    if len(estimators) != 2:\n","                        raise ValueError(\"model_fn returned a tuple of unexpected size ({} vs 2)\".format(len(estimators)))\n","\n","                    train_est, test_est = estimators\n","                    self.train_estimators[i] = train_est\n","                    self.test_estimators[i] = test_est\n","                else:\n","                    est = model_fn()\n","                    print('len of est ', len(est))\n","                    self.train_estimators[i] = est[0]\n","                    self.test_estimators[i] = est[1]\n","                    #print(f'train_estimators[{i}] : {est}')\n","                    #print(f'test_estimators[{i}] : {est}')\n","\n","        else:\n","            if (model_fn is None and num_estimators is None):   # assert raises error with integer value for num_estimators   # https://stackoverflow.com/questions/46850472/assert-using-python-how-do-i-check-if-the-input-is-integer-or-not\n","                raise AssertionError('assign values to model_fn and num_estimators')\n","\n","            self.train_estimators = models\n","            self.test_estimators = models\n","            self.num_estimators = len(models)\n","\n","\n","    def save(self, folder, filename_pattern=\"model-ensemble-{}.hdf5\"):\n","        \"\"\"\n","            Save a Deep Ensemble into a folder, using individual HDF5 files for each ensemble member.\n","            This allows for easily loading individual ensembles. Metadata is saved to allow loading of the whole ensemble.\n","        \"\"\"\n","\n","        if not os.path.exists(folder):\n","            os.makedirs(folder)\n","\n","        model_metadata = {}\n","\n","        for i in range(self.num_estimators):\n","            filename = os.path.join(folder, filename_pattern.format(i))\n","            self.test_estimators[i].save(filename)\n","\n","            print(\"Saved estimator {} to {}\".format(i, filename))\n","\n","            model_metadata[i] = filename_pattern.format(i)\n","\n","        metadata = {\"models\": model_metadata, \"class\": self.__module__}\n","\n","        with open(os.path.join(folder, METADATA_FILENAME), 'w') as outfile:\n","            yaml.dump(metadata, outfile)\n","            \n","\n","    @staticmethod\n","    def load(folder):\n","        \"\"\"\n","            Load a Deep Ensemble model from a folder containing individual HDF5 files.\n","        \"\"\"\n","        metadata = {}\n","\n","        with open(os.path.join(folder, METADATA_FILENAME)) as infile:\n","            metadata = yaml.full_load(infile)\n","\n","        models = []\n","\n","        for _, filename in metadata[\"models\"].items():\n","            models.append(keras.models.load_model(os.path.join(folder, filename)))\n","\n","        clazz = locate(metadata[\"class\"])\n","\n","        return clazz(models=models)  "],"metadata":{"id":"2RZG-PUMLK1v","executionInfo":{"status":"ok","timestamp":1648384822562,"user_tz":-120,"elapsed":420,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### DeepEnsembleRegressor"],"metadata":{"id":"WqKEst8GGdWT"}},{"cell_type":"code","source":["class DeepEnsembleRegressor(DeepEnsemble):\n","    \"\"\"\n","        Implementation of a Deep Ensemble for regression.\n","        Uses two models, one for training and another for inference/testing. The user has to provide a model function that returns\n","        the train and test models, and use the provided deep_ensemble_nll_loss for training.\n","    \"\"\"\n","    def __init__(self, model_fn=None, num_estimators=None, models=None):\n","        \"\"\"\n","            Builds a Deep Ensemble given a function to make model instances, and the number of estimators.\n","            For training it uses a model that only outputs the mean, while the loss uses both the mean and variance produced by the model.\n","            For testing, a model that shares weights with the training model is used, but the testing model outputs both mean and variance. The final\n","            prediction is made with a mixture of gaussians, where each gaussian is one trained model instance.\n","        \"\"\"\n","        super().__init__(model_fn=model_fn, num_estimators=num_estimators, models=models,\n","                         needs_test_estimators=True)\n","        \n","    def summary(self):\n","        print('training model summary ')\n","        print(self.model_fn()[0].summary())\n","        print('xxxxxxxxxxxxxxx')\n","        print('prediction model summary ')\n","        print(self.model_fn()[1].summary())\n","        \n","\n","    def fit(self, X, y, epochs=10, batch_size=32, **kwargs):\n","        \"\"\"\n","            Fits the Deep Ensemble, each estimator is fit independently on the same data.\n","        \"\"\"\n","\n","        for i in range(self.num_estimators):\n","            history = self.train_estimators[i].fit(X, y, epochs=epochs, batch_size=batch_size, **kwargs)\n","\n","            # plotting the training and validation curves\n","            plt.plot(history.history['loss'], label='train loss')\n","            plt.plot(history.history['val_loss'], label='val loss')\n","            plt.legend()\n","            plt.grid()\n","            plt.xlabel('epochs')\n","            plt.ylabel('loss')\n","            plt.title('loss curves')\n","            plt.show()\n","\n","            plt.plot(history.history['mae'], label='train mae')\n","            plt.plot(history.history['val_mae'], label='val mae')\n","            plt.legend()\n","            plt.grid()\n","            plt.xlabel('epochs')\n","            plt.ylabel('mae')\n","            plt.title('mae curves')\n","            plt.show()\n","            \n","    \n","    def fit_generator(self, generator, epochs=10, **kwargs):\n","        \"\"\"\n","            Fits the Deep Ensemble, each estimator is fit independently on the same data.\n","        \"\"\"\n","\n","        for i in range(self.num_estimators):\n","            self.train_estimators[i].fit_generator(generator, epochs=epochs, **kwargs)\n","            \n","\n","    def predict_output(self, X, batch_size=32, output_scaler=None, num_ensembles=None, disentangle_uncertainty=False, **kwargs):\n","        \"\"\"\n","            Makes a prediction. Predictions from each estimator are used to build a gaussian mixture and its mean and standard deviation returned.\n","        \"\"\"\n","        \n","        means = []\n","        variances = []\n","\n","        if num_ensembles is None:\n","            estimators = self.test_estimators\n","        else:\n","            estimators = self.test_estimators[:num_ensembles]\n","\n","        if \"verbose\" not in kwargs:\n","            kwargs[\"verbose\"] = 0\n","\n","        for estimator in estimators:\n","            mean, var  = estimator.predict(X, batch_size=batch_size, **kwargs)\n","\n","            if output_scaler is not None:\n","                mean = output_scaler.inverse_transform(mean)\n","\n","                # This should work but not sure if its 100% correct\n","                # Its not clear how to do inverse scaling of the variance\n","                sqrt_var = np.sqrt(var)\n","                var = output_scaler.inverse_transform(sqrt_var)\n","                var = np.square(var)\n","\n","            means.append(mean)\n","            variances.append(var)\n","\n","        means = np.array(means)\n","        variances = np.array(variances)\n","        \n","        mixture_mean = np.mean(means, axis=0)\n","        print('mixture_mean shape \\n', mixture_mean.shape)\n","        print('mixture_mean \\n', mixture_mean)\n","        mixture_var  = np.mean(variances + np.square(means), axis=0) - np.square(mixture_mean)\n","        mixture_var[mixture_var < 0.0] = 0.0\n","        print('mixture_var shape \\n', mixture_var.shape)\n","        print('mixture_var \\n', mixture_var)\n","                \n","        if disentangle_uncertainty:\n","            epi_var = np.var(means, axis=0)\n","            ale_var = np.mean(variances, axis=0)\n","\n","            return mixture_mean, np.sqrt(ale_var), np.sqrt(epi_var)\n","\n","        return mixture_mean, np.sqrt(mixture_var)\n","\n","    def predict_generator(self, generator, steps=None, num_ensembles=None, **kwargs):\n","        \"\"\"\n","            Makes a prediction. Predictions from each estimator are used to build a gaussian mixture and its mean and standard deviation returned.\n","        \"\"\"\n","        \n","        means = []\n","        variances = []\n","\n","        if num_ensembles is None:\n","            estimators = self.test_estimators\n","        else:\n","            estimators = self.test_estimators[:num_ensembles]\n","\n","        for estimator in estimators:\n","            mean, var  = estimator.predict_generator(generator, steps=steps, **kwargs)\n","            means.append(mean)\n","            variances.append(var)\n","\n","        means = np.array(means)\n","        variances = np.array(variances)\n","        \n","        mixture_mean = np.mean(means, axis=0)\n","        mixture_var  = np.mean(variances + np.square(means), axis=0) - np.square(mixture_mean)\n","        mixture_var[mixture_var < 0.0] = 0.0\n","                \n","        return mixture_mean, np.sqrt(mixture_var)"],"metadata":{"id":"fWBHurY2LSVU","executionInfo":{"status":"ok","timestamp":1648384822959,"user_tz":-120,"elapsed":399,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Load the saved model"],"metadata":{"id":"y5jnsDSyH_TH"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IYxC_5VkocsL","executionInfo":{"status":"ok","timestamp":1648384824900,"user_tz":-120,"elapsed":1950,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"20c2ebf6-0b0f-4746-cd0b-613e672a5e04"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MasterThesis/CaliforniaHousingDatasetTests/GBP_explanation/deep_ensemble/ensemble_model_epochs_5_num_estimators_5.h5\n","model-ensemble-0.hdf5\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","model-ensemble-1.hdf5\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","model-ensemble-2.hdf5\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","model-ensemble-3.hdf5\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","model-ensemble-4.hdf5\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","metadata.yml\n","total number of ensemble components :  5\n"]}],"source":["# slightly different procedure to load the ensemble model from yaml file \n","\n","import os\n","\n","'''\n","dir_name = '/content/drive/MyDrive/MasterThesis/CaliforniaHousingDatasetTests/GBP_explanation/deep_ensemble/'\n","test = os.listdir(dir_name)\n","\n","for item in test:\n","    if '.h5' in item:\n","        dir = dir_name+item\n","        with open(dir+'/model_as_json.json', 'r') as json_file:\n","            json_savedModel=json_file.read()\n","        model = tf.keras.models.model_from_json(json_savedModel)\n","\n","model.summary()\n","\n","# reference : https://towardsdatascience.com/saving-and-loading-keras-model-42195b92f57a\n","'''\n","\n","model = [] \n","num_estimators = 0 \n","for item in os.listdir(path):\n","    if '.h5' in item:\n","        print(path+item)\n","        for i in os.listdir(path+item):\n","            print(i)\n","            if '.hdf5' in i:\n","                model_variable = tf.keras.models.load_model(path+item+'/'+i)\n","                model.append(model_variable)\n","        #print('summary for estimator # ', num_estimators)\n","        #print(model[num_estimators].summary)\n","        num_estimators +=1\n","\n","print('total number of ensemble components : ', len(model))"]},{"cell_type":"markdown","source":["## Defining model_fn()"],"metadata":{"id":"OimVgPulREtf"}},{"cell_type":"code","source":["import keras \n","\n","from keras.layers import *\n","from keras.models import Model, Sequential\n","import keras.backend as K \n","\n","#obtained from hyperparameter optimization\n","def model_function():\n","    inp = Input(shape=(8,))\n","    x = Dense(32, activation='relu')(inp)\n","    x = Dense(32, activation='relu')(x)\n","    mean = Dense(1, activation='relu')(x)\n","    var = Dense(1, activation='softplus')(x)\n","\n","    train_model = Model(inp, mean)\n","    pred_model = Model(inp, [mean, var])\n","    #train_model.compile(loss='mse', optimizer='sgd', metrics=['mae'])   \n","    train_model.compile(loss=regression_gaussian_nll_loss(var), optimizer='adam', metrics=['mae'])\n","\n","\n","    return train_model, pred_model"],"metadata":{"id":"pbU69yIORH50","executionInfo":{"status":"ok","timestamp":1648384824902,"user_tz":-120,"elapsed":16,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Inference on test set"],"metadata":{"id":"1YvqeSzNOfJG"}},{"cell_type":"code","source":["\n","# Analysis of the input \n","#num_of_inputs_to_be_explained = 1\n","\n","#start_index = np.random.randint(0, test_data.shape[0])\n","#print('start_index : ', start_index)\n","\n","#test_input = test_data[start_index:start_index+num_of_inputs_to_be_explained]\n","#print('test_input shape : ', test_input.shape)\n","\n","#test_label = test_data[start_index:start_index+num_of_inputs_to_be_explained]\n","#print('test_label : ', test_data[start_index:start_index+num_of_inputs_to_be_explained])\n","\n","\n","#test_input_adj = np.expand_dims(test_input, axis=-1)\n","#print('test_input_adj shape :', test_input_adj.shape)\n","\n","\n","# MODEL PREDICTION AND PLOTTING \n","stochastic_model = DeepEnsembleRegressor(model_fn=model_function, num_estimators=len(model), models=model)\n","pred_mean, pred_std = stochastic_model.predict_output(test_data, num_ensembles=len(model)) #we calculate mix_Var but return sqrt of it hence catch it in pred_std\n","\n","print('pred_mean shape ', pred_mean.shape)\n","print('pred_std shape ', pred_std.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-0q9ikftFlr","executionInfo":{"status":"ok","timestamp":1648384838825,"user_tz":-120,"elapsed":1527,"user":{"displayName":"Mihir Mulye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjt-UFuHMJLt42FC-zaYNS5lke3QC-GYFy34wxbQw=s64","userId":"03370256418408681216"}},"outputId":"52bb968d-1a66-47fb-f2c1-0c02357f2629"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["mixture_mean shape \n"," (3000, 1)\n","mixture_mean \n"," [[0.5053374 ]\n"," [0.27416188]\n"," [0.35749024]\n"," ...\n"," [0.1446454 ]\n"," [0.17725742]\n"," [0.7458336 ]]\n","mixture_var shape \n"," (3000, 1)\n","mixture_var \n"," [[0.15106606]\n"," [0.05928881]\n"," [0.08749378]\n"," ...\n"," [0.02757661]\n"," [0.035914  ]\n"," [0.35274774]]\n","pred_mean shape  (3000, 1)\n","pred_std shape  (3000, 1)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"KrWYhFDqatv-"},"execution_count":null,"outputs":[]}]}